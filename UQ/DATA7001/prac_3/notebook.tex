
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Prac 3 - Data Transformation \& Exploration SOLUTIONS}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Part 1 Data Transformation}\label{part-1-data-transformation}

In computer programming, a \emph{data type} of a variable determines
what type of values it can contain and what operations can be performed.
The following table summarises some of the common data types we will
come across in R:

\begin{longtable}[]{@{}lll@{}}
\toprule
Data Type & Example & R Code\tabularnewline
\midrule
\endhead
Logical & TRUE, FALSE & \texttt{v\ \textless{}-\ TRUE}\tabularnewline
Numeric & 12.3, 5, 999 & \texttt{v\ \textless{}-\ 23.5}\tabularnewline
Integer & 1L, 34L, 0L & \texttt{v\ \textless{}-\ 2L}\tabularnewline
Character & "a", "good", "TRUE", '23.4' &
\texttt{v\ \textless{}-\ "TRUE"}\tabularnewline
\bottomrule
\end{longtable}

You can use the \texttt{class(var)} syntax to determine what object type
a variable is, as demonstrated in the following example:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} char\PYZus{}var \PY{o}{\PYZlt{}\PYZhy{}} \PY{l+s}{\PYZdq{}}\PY{l+s}{12345\PYZdq{}} 
        num\PYZus{}var \PY{o}{\PYZlt{}\PYZhy{}} \PY{l+m}{12345} 
        \PY{k+kp}{class}\PY{p}{(}char\PYZus{}var\PY{p}{)}
        \PY{k+kp}{class}\PY{p}{(}num\PYZus{}var\PY{p}{)}
\end{Verbatim}


    'character'

    
    'numeric'

    
    Having a dataset that contains inconsistent \emph{data types} is a
common data cleaning problem. The above example demonstrates two
different ways the number 12345 could be expressed in a dataset, as a
\texttt{character} or a \texttt{numeric} value. The data type of the
variable determines what sort of operations can be performed on them.

Datasets that are interpreted as the wrong data type, or that are
\emph{inconsistent} need to be cleaned so that the desired operations
can be performed on the dataset. If a column that is supposed to contain
integers contains characters, for example, we can no longer run numeric
functions such as \texttt{mean()} on those values:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} a \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kt}{c}\PY{p}{(}\PY{l+m}{15}\PY{p}{,} \PY{l+m}{20}\PY{p}{,} \PY{l+m}{1}\PY{p}{,} \PY{l+m}{10}\PY{p}{)}
        b \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{15\PYZdq{}}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{1\PYZdq{}}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{4\PYZdq{}}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} Valid! We can multiply numberic values together.}
        \PY{k+kp}{print}\PY{p}{(}\PY{k+kp}{mean}\PY{p}{(}a\PY{p}{)}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} You can\PYZsq{}t multiply a set of characters by a number.}
        \PY{k+kp}{print}\PY{p}{(}\PY{k+kp}{mean}\PY{p}{(}b\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[1] 11.5

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
Warning message in mean.default(b):
“argument is not numeric or logical: returning NA”
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
[1] NA

    \end{Verbatim}

    Luckily, R contains in-built functions designed to convert between
data-types. If we use the \texttt{as.numeric()} function, we can attempt
to convert a \texttt{character} to a \texttt{numeric\ type}. Let's try
the above example again, making sure we convert the characters to a
numeric value before attempting to run the \texttt{mean} function:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} b \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{15\PYZdq{}}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{1\PYZdq{}}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{4\PYZdq{}}\PY{p}{)}
        \PY{k+kp}{mean}\PY{p}{(}\PY{k+kp}{as.numeric}\PY{p}{(}b\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    6.66666666666667

    
    Converting between data-types is a common feature of
\emph{data-cleaning} and \emph{transformation}. Let's demonstrate by
cleaning an example dataset.

This dataset is a modified version of food-borne gastrointestinal
illness in the US in 1940. The data has been modified from the original
to include Brisbane-based addresses and some more obvious data integrity
issues have been injected.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{k+kn}{library}\PY{p}{(}readr\PY{p}{)}
        oswego \PY{o}{\PYZlt{}\PYZhy{}} read\PYZus{}csv\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{OswegoTutorial.csv\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Parsed with column specification:
cols(
  .default = col\_character(),
  sex = col\_integer(),
  timesupper = col\_integer(),
  onsettime = col\_integer()
)
See spec({\ldots}) for full column specifications.

    \end{Verbatim}

    Our first step might be to explore the data and get a feel for the type
of data we are working with. We can use the \texttt{head()} function to
have a look at the first few rows of the data:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k+kp}{head}\PY{p}{(}oswego\PY{p}{)}
\end{Verbatim}


    \begin{tabular}{r|lllllllllllllllllllll}
 age & sex & timesupper & ill & address & onsetdate & onsettime & bakedham & spinach & mashedpota & ⋯ & jello & rolls & brownbread & milk & coffee & water & cakes & vanilla & chocolate & fruitsalad\\
\hline
	 11                                                  & 2                                                   &   NA                                                & no                                                  & 7104 Legend Avenue, Waterfront Place, QLD, 4001     & NA                                                  &   NA                                                & no                                                  & no                                                  & no                                                  & ⋯                                                   & no                                                  & no                                                  & no                                                  & no                                                  & no                                                  & no                                                  & no                                                  & no                                                  & yes                                                 & no                                                 \\
	 52                                                  & 1                                                   & 2000                                                & yes                                                 & 7402 Woodhall Lane, Eagle Farm, QLD, 4009           & 19-Apr                                              &   30                                                & yes                                                 & yes                                                 & yes                                                 & ⋯                                                   & no                                                  & yes                                                 & no                                                  & no                                                  & yes                                                 & no                                                  & no                                                  & yes                                                 & no                                                  & no                                                 \\
	 65                                                  & 2                                                   & 1830                                                & yes                                                 & 7206 Nikerton Street, Wintergarden, QLD, 4002       & 19-Apr                                              &   30                                                & yes                                                 & yes                                                 & yes                                                 & ⋯                                                   & no                                                  & no                                                  & no                                                  & no                                                  & yes                                                 & no                                                  & no                                                  & yes                                                 & yes                                                 & no                                                 \\
	 59                                                  & 1                                                   & 1830                                                & yes                                                 & 579 Woodleaf Drive, Albion, QLD, 4010               & 19-Apr                                              &   30                                                & yes                                                 & yes                                                 & no                                                  & ⋯                                                   & no                                                  & no                                                  & no                                                  & no                                                  & yes                                                 & no                                                  & yes                                                 & yes                                                 & yes                                                 & no                                                 \\
	 13                                                  & 1                                                   &   NA                                                & no                                                  & 3611 Nottingham Oaks Avenue, Spring Hill, QLD, 4000 & NA                                                  &   NA                                                & no                                                  & no                                                  & no                                                  & ⋯                                                   & no                                                  & no                                                  & no                                                  & no                                                  & no                                                  & no                                                  & no                                                  & no                                                  & yes                                                 & no                                                 \\
	 63                                                  & 1                                                   & 1930                                                & yes                                                 & 5450 Cd Smith Way, New Farm, QLD, 4005              & 18-Apr                                              & 2230                                                & yes                                                 & yes                                                 & no                                                  & ⋯                                                   & yes                                                 & no                                                  & no                                                  & no                                                  & no                                                  & yes                                                 & no                                                  & yes                                                 & no                                                  & no                                                 \\
\end{tabular}


    
    If we want to determine how large the dataset is, we can use the
\texttt{dim()} function to determine the dimensions:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{k+kp}{dim}\PY{p}{(}oswego\PY{p}{)}
\end{Verbatim}


    \begin{enumerate*}
\item 76
\item 21
\end{enumerate*}


    
    Let's have a closer look at the "age" column:

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} oswego\PY{o}{\PYZdl{}}age
\end{Verbatim}


    \begin{enumerate*}
\item '11'
\item '52'
\item '65'
\item '59'
\item '13'
\item '63'
\item '70'
\item '40'
\item '15'
\item '33'
\item '65'
\item '38'
\item '62'
\item '10'
\item '25'
\item '32'
\item '62'
\item '36'
\item '11'
\item '33'
\item '13'
\item 'seven'
\item '64'
\item '3'
\item '65'
\item '59'
\item '15'
\item '622'
\item '37'
\item '17'
\item '35'
\item '15'
\item '50'
\item '40'
\item '35'
\item '35'
\item '36'
\item '57'
\item '16'
\item '68'
\item '54'
\item '77'
\item '72'
\item '58'
\item '20'
\item '17'
\item '62'
\item '20'
\item '52'
\item '9'
\item '50'
\item '8'
\item '35'
\item '48'
\item '25'
\item '11'
\item '74'
\item '12'
\item '44'
\item '53'
\item '37'
\item '24'
\item '69'
\item '7'
\item '17'
\item '8'
\item '11'
\item '17'
\item '36'
\item '21'
\item '60'
\item '18'
\item '14'
\item '52'
\item '45'
\item NA
\end{enumerate*}


    
    Do you notice anything odd about this row? It looks like there was a
data input error, and the number 7 has been inserted as the word
"seven". Let's see how this affects our data analysis by trying to run
the mean() function on the age column to determine the average age of
people in our dataset.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{k+kp}{mean}\PY{p}{(}oswego\PY{o}{\PYZdl{}}age\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Warning message in mean.default(oswego\$age):
“argument is not numeric or logical: returning NA”
    \end{Verbatim}

    <NA>

    
    As you can see, we get an error, saying that the "argument is not
numeric or logical". It looks like we can only run the mean function on
a column that is \emph{numeric} or \emph{logical}. What data type is the
age column?

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{k+kp}{class}\PY{p}{(}oswego\PY{o}{\PYZdl{}}age\PY{p}{)}
\end{Verbatim}


    'character'

    
    We can see that R has interpreted this column as the 'character' data
type, which explains why we can't run the mean function on it. Let's
first replace the character(s) "seven" with "7" so that we can easily
convert the whole column to a numeric data type.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} oswego\PY{o}{\PYZdl{}}age\PY{p}{[}oswego\PY{o}{\PYZdl{}}age \PY{o}{==} \PY{l+s}{\PYZdq{}}\PY{l+s}{seven\PYZdq{}}\PY{p}{]} \PY{o}{\PYZlt{}\PYZhy{}} \PY{l+s}{\PYZdq{}}\PY{l+s}{7\PYZdq{}}
\end{Verbatim}


    Let's break down the above query a little bit. The
\texttt{oswego\$age\ ==\ "seven"} is what is called a \emph{conditional
statement}, it matches values in \texttt{oswego\$age} according to a
specific condition. In this case, we match any of the rows that have the
value "seven". We then set any of these rows to the character "7"
instead.

    \textbar{}

TASK

\begin{longtable}[]{@{}l@{}}
\toprule
\begin{minipage}[t]{0.07\columnwidth}\raggedright\strut
Convert the age column to the correct data-type (numeric). What is the
mean of the ages?\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

    **Note: Never replace null values with 0

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} OmitNA\PY{o}{\PYZlt{}\PYZhy{}}na.omit\PY{p}{(}oswego\PY{o}{\PYZdl{}}age\PY{p}{)} \PY{c+c1}{\PYZsh{} The NA value is omitted and remaining data is stored in OmitNA}
         AsNum\PY{o}{\PYZlt{}\PYZhy{}}\PY{k+kp}{as.numeric}\PY{p}{(}OmitNA\PY{p}{)} \PY{c+c1}{\PYZsh{} OmitNA from previous step is stored as numberic in AsNum}
         \PY{k+kp}{mean}\PY{p}{(}AsNum\PY{p}{)} \PY{c+c1}{\PYZsh{} Mean is calculated}
\end{Verbatim}


    44.28

    
    You may have noticed that in our \texttt{oswego} dataset, we have the
full address of each patient. Can you think of how this may be useful?

One use of this locational data might be to see if outbreaks are
clustered around particular suburbs/areas. In this case, being able to
query the postcode directly would be useful, but currently the postcode
is within the 'Address' column, which has the format:

\begin{verbatim}
38 Jones Road, South Brisbane, QLD 4101
\end{verbatim}

One common aspect of preparing your data for use is making sure that it
is in a format that is as simple to query as possible. For example, in
the above case, if we wanted to find out what the most common suburb is
which contained an outbreak, we would not be able to easily query suburb
specifically with the \texttt{address} column as it contains a lot of
superflous information.

One solution might be to transform the data so each part of the address
is in its own column - that way we query against a much simpler
attribute such as postcode.

Let's demonstrate this by splitting up the "onsetdate" column first as
an example. You can see that the \texttt{onsetdate} is in the format:

\begin{verbatim}
19-Apr
\end{verbatim}

If we wanted to query directly by month, we could separate the "Month"
part of the address directly into its own column. Let's do that. We will
use the "tidyr" library in R, a popular data transformation library. Run
the code below and try and understand what it is doing.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{k+kn}{library}\PY{p}{(}tidyr\PY{p}{)}
         
         oswego\PYZus{}new \PY{o}{\PYZlt{}\PYZhy{}} separate\PY{p}{(}oswego\PY{p}{,} onsetdate\PY{p}{,} into \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{onset\PYZus{}day\PYZdq{}}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{onset\PYZus{}month\PYZdq{}}\PY{p}{)}\PY{p}{,} sep \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{\PYZhy{}\PYZdq{}}\PY{p}{)}
         \PY{k+kp}{head}\PY{p}{(}oswego\PYZus{}new\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Warning message:
“Expected 2 pieces. Missing pieces filled with `NA` in 1 rows [24].”
    \end{Verbatim}

    \begin{tabular}{r|llllllllllllllllllllll}
 age & sex & timesupper & ill & address & onset\_day & onset\_month & onsettime & bakedham & spinach & ⋯ & jello & rolls & brownbread & milk & coffee & water & cakes & vanilla & chocolate & fruitsalad\\
\hline
	 11                                                  & 2                                                   &   NA                                                & no                                                  & 7104 Legend Avenue, Waterfront Place, QLD, 4001     & NA                                                  & NA                                                  &   NA                                                & no                                                  & no                                                  & ⋯                                                   & no                                                  & no                                                  & no                                                  & no                                                  & no                                                  & no                                                  & no                                                  & no                                                  & yes                                                 & no                                                 \\
	 52                                                  & 1                                                   & 2000                                                & yes                                                 & 7402 Woodhall Lane, Eagle Farm, QLD, 4009           & 19                                                  & Apr                                                 &   30                                                & yes                                                 & yes                                                 & ⋯                                                   & no                                                  & yes                                                 & no                                                  & no                                                  & yes                                                 & no                                                  & no                                                  & yes                                                 & no                                                  & no                                                 \\
	 65                                                  & 2                                                   & 1830                                                & yes                                                 & 7206 Nikerton Street, Wintergarden, QLD, 4002       & 19                                                  & Apr                                                 &   30                                                & yes                                                 & yes                                                 & ⋯                                                   & no                                                  & no                                                  & no                                                  & no                                                  & yes                                                 & no                                                  & no                                                  & yes                                                 & yes                                                 & no                                                 \\
	 59                                                  & 1                                                   & 1830                                                & yes                                                 & 579 Woodleaf Drive, Albion, QLD, 4010               & 19                                                  & Apr                                                 &   30                                                & yes                                                 & yes                                                 & ⋯                                                   & no                                                  & no                                                  & no                                                  & no                                                  & yes                                                 & no                                                  & yes                                                 & yes                                                 & yes                                                 & no                                                 \\
	 13                                                  & 1                                                   &   NA                                                & no                                                  & 3611 Nottingham Oaks Avenue, Spring Hill, QLD, 4000 & NA                                                  & NA                                                  &   NA                                                & no                                                  & no                                                  & ⋯                                                   & no                                                  & no                                                  & no                                                  & no                                                  & no                                                  & no                                                  & no                                                  & no                                                  & yes                                                 & no                                                 \\
	 63                                                  & 1                                                   & 1930                                                & yes                                                 & 5450 Cd Smith Way, New Farm, QLD, 4005              & 18                                                  & Apr                                                 & 2230                                                & yes                                                 & yes                                                 & ⋯                                                   & yes                                                 & no                                                  & no                                                  & no                                                  & no                                                  & yes                                                 & no                                                  & yes                                                 & no                                                  & no                                                 \\
\end{tabular}


    
    Now that we've separated the columns, we can directly query "month" to
see the range of months that these outbreaks occurred.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{k+kp}{unique}\PY{p}{(}oswego\PYZus{}new\PY{o}{\PYZdl{}}onset\PYZus{}month\PY{p}{)}
\end{Verbatim}


    \begin{enumerate*}
\item NA
\item 'Apr'
\item 'Jun'
\end{enumerate*}


    
    We can see that this data is limited to April, June and also that a lot
of the onset date data is missing, resulting in 'NA' values. Now let's
see how we can get useful information from the location information.

    \textbar{}

TASK

\begin{longtable}[]{@{}l@{}}
\toprule
\begin{minipage}[t]{0.07\columnwidth}\raggedright\strut
Separate the "address" column into four new columns, \texttt{address},
\texttt{suburb\_name}, \texttt{state} and \texttt{postcode}\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{k+kn}{library}\PY{p}{(}tidyr\PY{p}{)}
         oswego\PYZus{}address\PYZus{}separate \PY{o}{\PYZlt{}\PYZhy{}} separate\PY{p}{(}oswego\PY{p}{,} address\PY{p}{,} into \PY{o}{=} \PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{address\PYZdq{}}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{suburb\PYZus{}name\PYZdq{}}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{state\PYZdq{}}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{postcode\PYZdq{}}\PY{p}{)}\PY{p}{,} sep \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{,\PYZdq{}}\PY{p}{)}
         \PY{k+kp}{head}\PY{p}{(}oswego\PYZus{}address\PYZus{}separate\PY{p}{)}
\end{Verbatim}


    \begin{tabular}{r|llllllllllllllllllllllll}
 age & sex & timesupper & ill & address & suburb\_name & state & postcode & onsetdate & onsettime & ⋯ & jello & rolls & brownbread & milk & coffee & water & cakes & vanilla & chocolate & fruitsalad\\
\hline
	 11                          & 2                           &   NA                        & no                          & 7104 Legend Avenue          &  Waterfront Place           &  QLD                        &  4001                       & NA                          &   NA                        & ⋯                           & no                          & no                          & no                          & no                          & no                          & no                          & no                          & no                          & yes                         & no                         \\
	 52                          & 1                           & 2000                        & yes                         & 7402 Woodhall Lane          &  Eagle Farm                 &  QLD                        &  4009                       & 19-Apr                      &   30                        & ⋯                           & no                          & yes                         & no                          & no                          & yes                         & no                          & no                          & yes                         & no                          & no                         \\
	 65                          & 2                           & 1830                        & yes                         & 7206 Nikerton Street        &  Wintergarden               &  QLD                        &  4002                       & 19-Apr                      &   30                        & ⋯                           & no                          & no                          & no                          & no                          & yes                         & no                          & no                          & yes                         & yes                         & no                         \\
	 59                          & 1                           & 1830                        & yes                         & 579 Woodleaf Drive          &  Albion                     &  QLD                        &  4010                       & 19-Apr                      &   30                        & ⋯                           & no                          & no                          & no                          & no                          & yes                         & no                          & yes                         & yes                         & yes                         & no                         \\
	 13                          & 1                           &   NA                        & no                          & 3611 Nottingham Oaks Avenue &  Spring Hill                &  QLD                        &  4000                       & NA                          &   NA                        & ⋯                           & no                          & no                          & no                          & no                          & no                          & no                          & no                          & no                          & yes                         & no                         \\
	 63                          & 1                           & 1930                        & yes                         & 5450 Cd Smith Way           &  New Farm                   &  QLD                        &  4005                       & 18-Apr                      & 2230                        & ⋯                           & yes                         & no                          & no                          & no                          & no                          & yes                         & no                          & yes                         & no                          & no                         \\
\end{tabular}


    
    \textbar{}

TASK

\begin{longtable}[]{@{}l@{}}
\toprule
\begin{minipage}[t]{0.07\columnwidth}\raggedright\strut
Which postcode has the most occurrences of illness? Remember, you will
need to filter your data by only those who are ill.\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{} Solution 1}
         \PY{c+c1}{\PYZsh{}filter data with those who are ill}
         oswego\PYZus{}ill \PY{o}{\PYZlt{}\PYZhy{}} oswego\PYZus{}address\PYZus{}separate\PY{p}{[}oswego\PYZus{}address\PYZus{}separate\PY{p}{[}\PY{p}{,}\PY{l+m}{4}\PY{p}{]}\PY{o}{==}\PY{l+s}{\PYZdq{}}\PY{l+s}{yes\PYZdq{}}\PY{p}{,}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{}aggregate by postcode}
         Postcode\PYZus{}Count \PY{o}{=} aggregate\PY{p}{(}ill\PY{o}{\PYZti{}}postcode\PY{p}{,} oswego\PYZus{}ill\PY{p}{,} \PY{k+kp}{length}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}find and display postcode with maximum occurances of ill}
         Postcode\PYZus{}Max \PY{o}{\PYZlt{}\PYZhy{}} Postcode\PYZus{}Count\PY{p}{[}Postcode\PYZus{}Count\PY{p}{[}\PY{p}{,}\PY{l+m}{2}\PY{p}{]}\PY{o}{==} \PY{k+kp}{max}\PY{p}{(}Postcode\PYZus{}Count\PY{o}{\PYZdl{}}ill\PY{p}{)}\PY{p}{,} \PY{p}{]} 
         Postcode\PYZus{}Max
         
         \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{} Solution 2}
         \PY{c+c1}{\PYZsh{}filter data with those who are ill}
         oswego\PYZus{}ill \PY{o}{\PYZlt{}\PYZhy{}} oswego\PYZus{}address\PYZus{}separate\PY{p}{[}oswego\PYZus{}address\PYZus{}separate\PY{p}{[}\PY{p}{,}\PY{l+m}{4}\PY{p}{]}\PY{o}{==}\PY{l+s}{\PYZdq{}}\PY{l+s}{yes\PYZdq{}}\PY{p}{,}\PY{p}{]}
         
         \PY{c+c1}{\PYZsh{}generate a frequency table}
         freqs \PY{o}{=} \PY{k+kp}{table}\PY{p}{(}oswego\PYZus{}ill\PY{p}{[}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{postcode\PYZsq{}}\PY{p}{]}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{}find and display postcode with maximum occurrances of ill}
         freqs\PY{p}{[}freqs\PY{o}{==}\PY{k+kp}{max}\PY{p}{(}freqs\PY{p}{)}\PY{p}{]}
         \PY{c+c1}{\PYZsh{}also try which(freqs==max(freqs)), and names(which(freqs==max(freqs)))}
\end{Verbatim}


    \begin{tabular}{r|ll}
  & postcode & ill\\
\hline
	6 &  4006 & 5    \\
	9 &  4010 & 5    \\
\end{tabular}


    
    
    \begin{verbatim}

 4006  4010 
    5     5 
    \end{verbatim}

    
    \textbar{}

EXTENSION TASK

\begin{longtable}[]{@{}l@{}}
\toprule
\begin{minipage}[t]{0.07\columnwidth}\raggedright\strut
List all postcodes and number of occurances using (1) R and (2) MySQL
(tip: use group by query).\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} Postcode\PYZus{}Count
\end{Verbatim}


    \begin{tabular}{r|ll}
 postcode & ill\\
\hline
	  4000 & 2    \\
	  4001 & 2    \\
	  4002 & 1    \\
	  4004 & 3    \\
	  4005 & 1    \\
	  4006 & 5    \\
	  4007 & 3    \\
	  4009 & 2    \\
	  4010 & 5    \\
	  4012 & 1    \\
	  4013 & 1    \\
	  4014 & 2    \\
	  4017 & 3    \\
	  4020 & 3    \\
	  4021 & 3    \\
	  4022 & 2    \\
	  4025 & 4    \\
	  4029 & 2    \\
	  4030 & 1    \\
\end{tabular}


    
    \section{Part 2 Data Exploration}\label{part-2-data-exploration}

In Prac. 1, we first encountered the HR Analytics dataset (sourced from
Kaggle https://www.kaggle.com/ludobenistant/hr-analytics ).

The question we seek to answer is: \emph{Why are our best and most
experienced employees leaving prematurely?}

To get to grips with the data, we will carry out some exploratory data
analysis (EDA) techniques in R.

Firstly, let's import the data and look at a few rows.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{k+kn}{library}\PY{p}{(}readr\PY{p}{)}
         HR\PYZus{}comma\PYZus{}sep \PY{o}{\PYZlt{}\PYZhy{}} read\PYZus{}csv\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{https://stluc.manta.uqcloud.net/mdatascience/public/datasets/HumanResourceAnalytics/HR\PYZus{}comma\PYZus{}sep.csv\PYZdq{}}\PY{p}{)}
         HR\PYZus{}comma\PYZus{}sep
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Parsed with column specification:
cols(
  satisfaction\_level = col\_double(),
  last\_evaluation = col\_double(),
  number\_project = col\_integer(),
  average\_montly\_hours = col\_integer(),
  time\_spend\_company = col\_integer(),
  Work\_accident = col\_integer(),
  left = col\_integer(),
  promotion\_last\_5years = col\_integer(),
  sales = col\_character(),
  salary = col\_character()
)

    \end{Verbatim}

    \begin{tabular}{r|llllllllll}
 satisfaction\_level & last\_evaluation & number\_project & average\_montly\_hours & time\_spend\_company & Work\_accident & left & promotion\_last\_5years & sales & salary\\
\hline
	 0.38       & 0.53       & 2          & 157        & 3          & 0          & 1          & 0          & sales      & low       \\
	 0.80       & 0.86       & 5          & 262        & 6          & 0          & 1          & 0          & sales      & medium    \\
	 0.11       & 0.88       & 7          & 272        & 4          & 0          & 1          & 0          & sales      & medium    \\
	 0.72       & 0.87       & 5          & 223        & 5          & 0          & 1          & 0          & sales      & low       \\
	 0.37       & 0.52       & 2          & 159        & 3          & 0          & 1          & 0          & sales      & low       \\
	 0.41       & 0.50       & 2          & 153        & 3          & 0          & 1          & 0          & sales      & low       \\
	 0.10       & 0.77       & 6          & 247        & 4          & 0          & 1          & 0          & sales      & low       \\
	 0.92       & 0.85       & 5          & 259        & 5          & 0          & 1          & 0          & sales      & low       \\
	 0.89       & 1.00       & 5          & 224        & 5          & 0          & 1          & 0          & sales      & low       \\
	 0.42       & 0.53       & 2          & 142        & 3          & 0          & 1          & 0          & sales      & low       \\
	 0.45       & 0.54       & 2          & 135        & 3          & 0          & 1          & 0          & sales      & low       \\
	 0.11       & 0.81       & 6          & 305        & 4          & 0          & 1          & 0          & sales      & low       \\
	 0.84       & 0.92       & 4          & 234        & 5          & 0          & 1          & 0          & sales      & low       \\
	 0.41       & 0.55       & 2          & 148        & 3          & 0          & 1          & 0          & sales      & low       \\
	 0.36       & 0.56       & 2          & 137        & 3          & 0          & 1          & 0          & sales      & low       \\
	 0.38       & 0.54       & 2          & 143        & 3          & 0          & 1          & 0          & sales      & low       \\
	 0.45       & 0.47       & 2          & 160        & 3          & 0          & 1          & 0          & sales      & low       \\
	 0.78       & 0.99       & 4          & 255        & 6          & 0          & 1          & 0          & sales      & low       \\
	 0.45       & 0.51       & 2          & 160        & 3          & 1          & 1          & 1          & sales      & low       \\
	 0.76       & 0.89       & 5          & 262        & 5          & 0          & 1          & 0          & sales      & low       \\
	 0.11       & 0.83       & 6          & 282        & 4          & 0          & 1          & 0          & sales      & low       \\
	 0.38       & 0.55       & 2          & 147        & 3          & 0          & 1          & 0          & sales      & low       \\
	 0.09       & 0.95       & 6          & 304        & 4          & 0          & 1          & 0          & sales      & low       \\
	 0.46       & 0.57       & 2          & 139        & 3          & 0          & 1          & 0          & sales      & low       \\
	 0.40       & 0.53       & 2          & 158        & 3          & 0          & 1          & 0          & sales      & low       \\
	 0.89       & 0.92       & 5          & 242        & 5          & 0          & 1          & 0          & sales      & low       \\
	 0.82       & 0.87       & 4          & 239        & 5          & 0          & 1          & 0          & sales      & low       \\
	 0.40       & 0.49       & 2          & 135        & 3          & 0          & 1          & 0          & sales      & low       \\
	 0.41       & 0.46       & 2          & 128        & 3          & 0          & 1          & 0          & accounting & low       \\
	 0.38       & 0.50       & 2          & 132        & 3          & 0          & 1          & 0          & accounting & low       \\
	 ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮\\
	 0.43       & 0.46       & 2          & 157        & 3          & 0          & 1          & 0          & sales      & medium    \\
	 0.78       & 0.93       & 4          & 225        & 5          & 0          & 1          & 0          & sales      & medium    \\
	 0.39       & 0.45       & 2          & 140        & 3          & 0          & 1          & 0          & sales      & medium    \\
	 0.11       & 0.97       & 6          & 310        & 4          & 0          & 1          & 0          & accounting & medium    \\
	 0.36       & 0.52       & 2          & 143        & 3          & 0          & 1          & 0          & accounting & medium    \\
	 0.36       & 0.54       & 2          & 153        & 3          & 0          & 1          & 0          & accounting & medium    \\
	 0.10       & 0.79       & 7          & 310        & 4          & 0          & 1          & 0          & hr         & medium    \\
	 0.40       & 0.47       & 2          & 136        & 3          & 0          & 1          & 0          & hr         & medium    \\
	 0.81       & 0.85       & 4          & 251        & 6          & 0          & 1          & 0          & hr         & medium    \\
	 0.40       & 0.47       & 2          & 144        & 3          & 0          & 1          & 0          & hr         & medium    \\
	 0.09       & 0.93       & 6          & 296        & 4          & 0          & 1          & 0          & technical  & medium    \\
	 0.76       & 0.89       & 5          & 238        & 5          & 0          & 1          & 0          & technical  & high      \\
	 0.73       & 0.93       & 5          & 162        & 4          & 0          & 1          & 0          & technical  & low       \\
	 0.38       & 0.49       & 2          & 137        & 3          & 0          & 1          & 0          & technical  & medium    \\
	 0.72       & 0.84       & 5          & 257        & 5          & 0          & 1          & 0          & technical  & medium    \\
	 0.40       & 0.56       & 2          & 148        & 3          & 0          & 1          & 0          & technical  & medium    \\
	 0.91       & 0.99       & 5          & 254        & 5          & 0          & 1          & 0          & technical  & medium    \\
	 0.85       & 0.85       & 4          & 247        & 6          & 0          & 1          & 0          & technical  & low       \\
	 0.90       & 0.70       & 5          & 206        & 4          & 0          & 1          & 0          & technical  & low       \\
	 0.46       & 0.55       & 2          & 145        & 3          & 0          & 1          & 0          & technical  & low       \\
	 0.43       & 0.57       & 2          & 159        & 3          & 1          & 1          & 0          & technical  & low       \\
	 0.89       & 0.88       & 5          & 228        & 5          & 1          & 1          & 0          & support    & low       \\
	 0.09       & 0.81       & 6          & 257        & 4          & 0          & 1          & 0          & support    & low       \\
	 0.40       & 0.48       & 2          & 155        & 3          & 0          & 1          & 0          & support    & low       \\
	 0.76       & 0.83       & 6          & 293        & 6          & 0          & 1          & 0          & support    & low       \\
	 0.40       & 0.57       & 2          & 151        & 3          & 0          & 1          & 0          & support    & low       \\
	 0.37       & 0.48       & 2          & 160        & 3          & 0          & 1          & 0          & support    & low       \\
	 0.37       & 0.53       & 2          & 143        & 3          & 0          & 1          & 0          & support    & low       \\
	 0.11       & 0.96       & 6          & 280        & 4          & 0          & 1          & 0          & support    & low       \\
	 0.37       & 0.52       & 2          & 158        & 3          & 0          & 1          & 0          & support    & low       \\
\end{tabular}


    
    \textbar{}

TASK

\begin{longtable}[]{@{}l@{}}
\toprule
What type of variable is \texttt{left}?\tabularnewline
\bottomrule
\end{longtable}

    The "left" column hosts integer variables, which is of course
quantitative and discrete.

    Now, let's count the number of rows with missing data.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{k+kp}{sum}\PY{p}{(}\PY{o}{!}complete.cases\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    0

    
    Since there is no missing data, we can proceed with our analysis on the
complete data set.

Let's explore some simple quantitative summaries. The default summary
statistics are as follows.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{k+kp}{summary}\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
 satisfaction_level last_evaluation  number_project  average_montly_hours
 Min.   :0.0900     Min.   :0.3600   Min.   :2.000   Min.   : 96.0       
 1st Qu.:0.4400     1st Qu.:0.5600   1st Qu.:3.000   1st Qu.:156.0       
 Median :0.6400     Median :0.7200   Median :4.000   Median :200.0       
 Mean   :0.6128     Mean   :0.7161   Mean   :3.803   Mean   :201.1       
 3rd Qu.:0.8200     3rd Qu.:0.8700   3rd Qu.:5.000   3rd Qu.:245.0       
 Max.   :1.0000     Max.   :1.0000   Max.   :7.000   Max.   :310.0       
 time_spend_company Work_accident         left        promotion_last_5years
 Min.   : 2.000     Min.   :0.0000   Min.   :0.0000   Min.   :0.00000      
 1st Qu.: 3.000     1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.00000      
 Median : 3.000     Median :0.0000   Median :0.0000   Median :0.00000      
 Mean   : 3.498     Mean   :0.1446   Mean   :0.2381   Mean   :0.02127      
 3rd Qu.: 4.000     3rd Qu.:0.0000   3rd Qu.:0.0000   3rd Qu.:0.00000      
 Max.   :10.000     Max.   :1.0000   Max.   :1.0000   Max.   :1.00000      
    sales              salary         
 Length:14999       Length:14999      
 Class :character   Class :character  
 Mode  :character   Mode  :character  
                                      
                                      
                                      
    \end{verbatim}

    
    These statistics tell us a bit about each variable in isolation; what we
would like to do is obtain statistics pertinent to our question.

To proceed, it is useful to classify each variable as either a
\emph{response} or a \emph{predictor}. For this problem and data set, it
is clear that \texttt{left} is the response and all other variables are
predictors.

For example: What is the breakdown of \texttt{left} by job Department
(\texttt{sales})?

This is easily achieved by creating a \emph{two-way contingency} table,
which counts the number of intances in each \texttt{left} by
\texttt{sales} cell in a two by two table.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} test\PYZus{}table\PY{o}{\PYZlt{}\PYZhy{}}\PY{k+kp}{table}\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}sales\PY{p}{,}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{p}{)}
         test\PYZus{}table
\end{Verbatim}


    
    \begin{verbatim}
             
                 0    1
  accounting   563  204
  hr           524  215
  IT           954  273
  management   539   91
  marketing    655  203
  product_mng  704  198
  RandD        666  121
  sales       3126 1014
  support     1674  555
  technical   2023  697
    \end{verbatim}

    
    Marginal counts and proportions are easily created as follows.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{k+kp}{margin.table}\PY{p}{(}test\PYZus{}table\PY{p}{)} 
         \PY{k+kp}{margin.table}\PY{p}{(}test\PYZus{}table\PY{p}{,}\PY{l+m}{1}\PY{p}{)}
         \PY{k+kp}{margin.table}\PY{p}{(}test\PYZus{}table\PY{p}{,}\PY{l+m}{2}\PY{p}{)}
         
         \PY{k+kp}{prop.table}\PY{p}{(}test\PYZus{}table\PY{p}{)}
         \PY{k+kp}{prop.table}\PY{p}{(}test\PYZus{}table\PY{p}{,}\PY{l+m}{1}\PY{p}{)}
         \PY{k+kp}{prop.table}\PY{p}{(}test\PYZus{}table\PY{p}{,}\PY{l+m}{2}\PY{p}{)}
\end{Verbatim}


    14999

    
    
    \begin{verbatim}

 accounting          hr          IT  management   marketing product_mng 
        767         739        1227         630         858         902 
      RandD       sales     support   technical 
        787        4140        2229        2720 
    \end{verbatim}

    
    
    \begin{verbatim}

    0     1 
11428  3571 
    \end{verbatim}

    
    
    \begin{verbatim}
             
                        0           1
  accounting  0.037535836 0.013600907
  hr          0.034935662 0.014334289
  IT          0.063604240 0.018201213
  management  0.035935729 0.006067071
  marketing   0.043669578 0.013534236
  product_mng 0.046936462 0.013200880
  RandD       0.044402960 0.008067204
  sales       0.208413894 0.067604507
  support     0.111607440 0.037002467
  technical   0.134875658 0.046469765
    \end{verbatim}

    
    
    \begin{verbatim}
             
                      0         1
  accounting  0.7340287 0.2659713
  hr          0.7090663 0.2909337
  IT          0.7775061 0.2224939
  management  0.8555556 0.1444444
  marketing   0.7634033 0.2365967
  product_mng 0.7804878 0.2195122
  RandD       0.8462516 0.1537484
  sales       0.7550725 0.2449275
  support     0.7510094 0.2489906
  technical   0.7437500 0.2562500
    \end{verbatim}

    
    
    \begin{verbatim}
             
                       0          1
  accounting  0.04926496 0.05712686
  hr          0.04585229 0.06020722
  IT          0.08347917 0.07644917
  management  0.04716486 0.02548306
  marketing   0.05731537 0.05684682
  product_mng 0.06160308 0.05544665
  RandD       0.05827791 0.03388407
  sales       0.27353868 0.28395407
  support     0.14648232 0.15541865
  technical   0.17702135 0.19518342
    \end{verbatim}

    
    These proportions can also be conveniently visualised using a mosaic
plot, where the widths of rectangles are proportional to the number of
observations in the x variable categories (\texttt{Department}), and,
for each x variable category, the heights are proportional to the number
of observations in the corresponding y variable categories
(\texttt{Left}).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} mosaicplot\PY{p}{(}test\PYZus{}table\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Mosaic Plot of Left by Department\PYZdq{}}\PY{p}{,}xlab\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Department\PYZdq{}}\PY{p}{,}ylab\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Left\PYZdq{}}\PY{p}{,}las\PY{o}{=}\PY{l+m}{2}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_48_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \textbar{}

TASK

\begin{longtable}[]{@{}l@{}}
\toprule
\begin{minipage}[t]{0.07\columnwidth}\raggedright\strut
Which Department has the highest proportion of employees that have left?
Which Department has the lowest?\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

    Relative to Department: HR has the highest proportion of employees that
have left. Management has the lowest proportion of employees that have
left.

    How about the breakdown of average hours worked per month by left?

We will first create visual summaries using boxplots (which display
summary statistics visually).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} boxplot\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}average\PYZus{}montly\PYZus{}hours\PY{o}{\PYZti{}}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{p}{,}xlab\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Left\PYZdq{}}\PY{p}{,}ylab\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Average Monthly Hours\PYZdq{}}\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Boxplot of Average Monthly Hours by Left\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_52_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    This simple visual summary suggests that employees that left typically
worked longer hours.

We can get a more detailed view by constructing a histogram as follows.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} hist\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}average\PYZus{}montly\PYZus{}hours\PY{p}{[}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{o}{==}\PY{l+m}{1}\PY{p}{]}\PY{p}{,} col\PY{o}{=}rgb\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Histogram for Average Monthly Hours by Left\PYZdq{}}\PY{p}{,} xlab\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Average Monthly Hours\PYZdq{}}\PY{p}{)}
         hist\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}average\PYZus{}montly\PYZus{}hours\PY{p}{[}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{o}{==}\PY{l+m}{0}\PY{p}{]}\PY{p}{,} col\PY{o}{=}rgb\PY{p}{(}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{,}add\PY{o}{=}\PY{k+kc}{TRUE}\PY{p}{)}
         legend\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{topright\PYZdq{}}\PY{p}{,} \PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Not Left\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{Left\PYZdq{}}\PY{p}{)}\PY{p}{,}fill\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}rgb\PY{p}{(}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{,} rgb\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_54_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The histogram reveals an interesting feature of the data; the
distribution of average monthly hours for employees that left is
\emph{multimodal}. This suggests employees that leave fall into two
groups: those that work normal hours and those that work long hours.

The same information can be seen from a plot of the empirical cumulative
distribution function (ecdf) for average monthly hours by left.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} E0\PY{o}{\PYZlt{}\PYZhy{}}ecdf\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}average\PYZus{}montly\PYZus{}hours\PY{p}{[}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{o}{==}\PY{l+m}{0}\PY{p}{]}\PY{p}{)}
         E1\PY{o}{\PYZlt{}\PYZhy{}}ecdf\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}average\PYZus{}montly\PYZus{}hours\PY{p}{[}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{o}{==}\PY{l+m}{1}\PY{p}{]}\PY{p}{)}
         plot\PY{p}{(}E0\PY{p}{,}col\PY{o}{=}rgb\PY{p}{(}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{,}verticals \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,} do.points \PY{o}{=} \PY{k+kc}{FALSE}\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{ECDF for Average Monthly Hours by Left\PYZdq{}}\PY{p}{,} xlab\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Average Monthly Hours\PYZdq{}}\PY{p}{)}
         plot\PY{p}{(}E1\PY{p}{,}col\PY{o}{=}rgb\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{,}verticals \PY{o}{=} \PY{k+kc}{TRUE}\PY{p}{,} do.points \PY{o}{=} \PY{k+kc}{FALSE}\PY{p}{,}add\PY{o}{=}\PY{k+kc}{TRUE}\PY{p}{)}
         legend\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{bottomright\PYZdq{}}\PY{p}{,} \PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Not Left\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{Left\PYZdq{}}\PY{p}{)}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{1}\PY{p}{,} lty\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{1}\PY{p}{)}\PY{p}{,}col\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}rgb\PY{p}{(}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{,} rgb\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_56_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    We can also look at the time spent in the company by left.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}26}]:} par\PY{p}{(}mfrow\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}\PY{l+m}{2}\PY{p}{,}\PY{l+m}{1}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} Create 2 by 1 figure}
         \PY{c+c1}{\PYZsh{} Plot Frequencies}
         hist\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}time\PYZus{}spend\PYZus{}company\PY{p}{[}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{o}{==}\PY{l+m}{0}\PY{p}{]}\PY{p}{,}breaks\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}\PY{l+m}{0.5}\PY{p}{,}\PY{l+m}{1.5}\PY{p}{,}\PY{l+m}{2.5}\PY{p}{,}\PY{l+m}{3.5}\PY{p}{,}\PY{l+m}{4.5}\PY{p}{,}\PY{l+m}{5.5}\PY{p}{,}\PY{l+m}{6.5}\PY{p}{,}\PY{l+m}{7.5}\PY{p}{,}\PY{l+m}{8.5}\PY{p}{,}\PY{l+m}{9.5}\PY{p}{,}\PY{l+m}{10.5}\PY{p}{)}\PY{p}{,}col\PY{o}{=}rgb\PY{p}{(}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Histogram for Time Spent at Company by Left\PYZdq{}}\PY{p}{,} xlab\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Time Spent at Company (Years)\PYZdq{}}\PY{p}{,}freq\PY{o}{=}\PY{k+kc}{TRUE}\PY{p}{)}
         hist\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}time\PYZus{}spend\PYZus{}company\PY{p}{[}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{o}{==}\PY{l+m}{1}\PY{p}{]}\PY{p}{,}breaks\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}\PY{l+m}{0.5}\PY{p}{,}\PY{l+m}{1.5}\PY{p}{,}\PY{l+m}{2.5}\PY{p}{,}\PY{l+m}{3.5}\PY{p}{,}\PY{l+m}{4.5}\PY{p}{,}\PY{l+m}{5.5}\PY{p}{,}\PY{l+m}{6.5}\PY{p}{,}\PY{l+m}{7.5}\PY{p}{,}\PY{l+m}{8.5}\PY{p}{,}\PY{l+m}{9.5}\PY{p}{,}\PY{l+m}{10.5}\PY{p}{)}\PY{p}{,}col\PY{o}{=}rgb\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{,}add\PY{o}{=}\PY{k+kc}{TRUE}\PY{p}{,}freq\PY{o}{=}\PY{k+kc}{TRUE}\PY{p}{)}
         legend\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{topright\PYZdq{}}\PY{p}{,} \PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Not Left\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{Left\PYZdq{}}\PY{p}{)}\PY{p}{,}fill\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}rgb\PY{p}{(}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{,} rgb\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Plot Proportions}
         hist\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}time\PYZus{}spend\PYZus{}company\PY{p}{[}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{o}{==}\PY{l+m}{1}\PY{p}{]}\PY{p}{,}breaks\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}\PY{l+m}{0.5}\PY{p}{,}\PY{l+m}{1.5}\PY{p}{,}\PY{l+m}{2.5}\PY{p}{,}\PY{l+m}{3.5}\PY{p}{,}\PY{l+m}{4.5}\PY{p}{,}\PY{l+m}{5.5}\PY{p}{,}\PY{l+m}{6.5}\PY{p}{,}\PY{l+m}{7.5}\PY{p}{,}\PY{l+m}{8.5}\PY{p}{,}\PY{l+m}{9.5}\PY{p}{,}\PY{l+m}{10.5}\PY{p}{)}\PY{p}{,}col\PY{o}{=}rgb\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Histogram for Time Spent at Company by Left\PYZdq{}}\PY{p}{,} xlab\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Time Spent at Company (Years)\PYZdq{}}\PY{p}{,}freq\PY{o}{=}\PY{k+kc}{FALSE}\PY{p}{)}
         hist\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}time\PYZus{}spend\PYZus{}company\PY{p}{[}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{o}{==}\PY{l+m}{0}\PY{p}{]}\PY{p}{,}breaks\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}\PY{l+m}{0.5}\PY{p}{,}\PY{l+m}{1.5}\PY{p}{,}\PY{l+m}{2.5}\PY{p}{,}\PY{l+m}{3.5}\PY{p}{,}\PY{l+m}{4.5}\PY{p}{,}\PY{l+m}{5.5}\PY{p}{,}\PY{l+m}{6.5}\PY{p}{,}\PY{l+m}{7.5}\PY{p}{,}\PY{l+m}{8.5}\PY{p}{,}\PY{l+m}{9.5}\PY{p}{,}\PY{l+m}{10.5}\PY{p}{)}\PY{p}{,}col\PY{o}{=}rgb\PY{p}{(}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{,}add\PY{o}{=}\PY{k+kc}{TRUE}\PY{p}{,}freq\PY{o}{=}\PY{k+kc}{FALSE}\PY{p}{)}
         legend\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{topright\PYZdq{}}\PY{p}{,} \PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Not Left\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{Left\PYZdq{}}\PY{p}{)}\PY{p}{,}fill\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}rgb\PY{p}{(}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{,} rgb\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_58_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    We see from the above plots that the densities (\texttt{Density}) are
easier than the frequency counts to visually compare, although relative
magnitude information is lost in the process.

The shape of the distributions for time spent at the company are
noticably different, with employees that have been at the company for
very short of very long periods of time being less likely to leave. Most
employees that leave the company have worked there for between three and
five years, inclusive.

    \textbar{}

TASK

\begin{longtable}[]{@{}l@{}}
\toprule
\begin{minipage}[t]{0.07\columnwidth}\raggedright\strut
Construct a histogram for last evaluation by left. What is a possible
explanation for any patterns you see?\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}27}]:} \PY{c+c1}{\PYZsh{} Plot Density}
         hist\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}last\PYZus{}evaluation\PY{p}{[}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{o}{==}\PY{l+m}{1}\PY{p}{]}\PY{p}{,}col\PY{o}{=}rgb\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Histogram for Last Evaluation at Company by Left\PYZdq{}}\PY{p}{,} xlab\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Last Evaluation\PYZdq{}}\PY{p}{,}freq\PY{o}{=}\PY{k+kc}{FALSE}\PY{p}{)}
         hist\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}last\PYZus{}evaluation\PY{p}{[}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{o}{==}\PY{l+m}{0}\PY{p}{]}\PY{p}{,}col\PY{o}{=}rgb\PY{p}{(}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{,}add\PY{o}{=}\PY{k+kc}{TRUE}\PY{p}{,}freq\PY{o}{=}\PY{k+kc}{FALSE}\PY{p}{)}
         legend\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{topright\PYZdq{}}\PY{p}{,} \PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Not Left\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{Left\PYZdq{}}\PY{p}{)}\PY{p}{,}fill\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}rgb\PY{p}{(}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{,} rgb\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_61_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Multimodal pattern, reasons why people are leaving if they were
evaluated highly - perhaps believed their compensation after evaluation
was insignificant, or people that left when their evaluation was bad
left also possibly left for similiar reasons, as there is an implicit
feeling of undervaluedness.

    Suppose we wish to plot last evaluation and average monthly hours by
leave. One way to do this is with a scatter plot.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}28}]:} plot\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}average\PYZus{}montly\PYZus{}hours\PY{p}{[}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{o}{==}\PY{l+m}{0}\PY{p}{]}\PY{p}{,}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}last\PYZus{}evaluation\PY{p}{[}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{o}{==}\PY{l+m}{0}\PY{p}{]}\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Scatter Plot for Average Monthly Hours vs \PYZbs{}n Last Evalation by Left\PYZdq{}}\PY{p}{,} xlab\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Average Monthly Hours\PYZdq{}}\PY{p}{,}ylab\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Last Evaluation\PYZdq{}}\PY{p}{,}col\PY{o}{=}rgb\PY{p}{(}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{)}
         points\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}average\PYZus{}montly\PYZus{}hours\PY{p}{[}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{o}{==}\PY{l+m}{1}\PY{p}{]}\PY{p}{,}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}last\PYZus{}evaluation\PY{p}{[}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{o}{==}\PY{l+m}{1}\PY{p}{]}\PY{p}{,}col\PY{o}{=}rgb\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{)}
         legend\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{bottomright\PYZdq{}}\PY{p}{,} \PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Not Left\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{Left\PYZdq{}}\PY{p}{)}\PY{p}{,}pch\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{1}\PY{p}{)}\PY{p}{,}col\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}rgb\PY{p}{(}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{,} rgb\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_64_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    From the scatter plot, we gain additional insight into the relationship
between last evaluation, average monthly hours, and those that left. The
bulk of those leaving that worked normal hours also received low
evaluations, and most of those leaving that worked long hours received
high evaluations.

We can break down the scatterplot further by an additional variable with
the construction of a scatter plot matrix. For instance, we might be
curious how satisfaction level, last evaluation, average monthly hours,
and left relate to eachother.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}29}]:} \PY{k+kn}{library}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{lattice\PYZdq{}}\PY{p}{)} \PY{c+c1}{\PYZsh{}Load the `Lattice\PYZsq{} graphics package}
         HR\PYZus{}subset\PY{o}{\PYZlt{}\PYZhy{}}\PY{k+kp}{subset}\PY{p}{(}\PY{k+kp}{as.data.frame}\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{p}{)}\PY{p}{,}select\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{average\PYZus{}montly\PYZus{}hours\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{last\PYZus{}evaluation\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{satisfaction\PYZus{}level\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{left\PYZsq{}}\PY{p}{)}\PY{p}{)}
         super.sym \PY{o}{\PYZlt{}\PYZhy{}} trellis.par.get\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{superpose.symbol\PYZdq{}}\PY{p}{)} \PY{c+c1}{\PYZsh{}Get Symbol Plotting Information}
         splom\PY{p}{(}\PY{o}{\PYZti{}}HR\PYZus{}subset\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{3}\PY{p}{]}\PY{p}{,}groups\PY{o}{=}left\PY{p}{,}data \PY{o}{=} HR\PYZus{}subset\PY{p}{,}varnames\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Average \PYZbs{}nMonthly \PYZbs{}nHours\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{Last \PYZbs{}nEvaluation\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{Satisfaction \PYZbs{}nLevel\PYZdq{}}\PY{p}{)}\PY{p}{,}
               key \PY{o}{=} \PY{k+kt}{list}\PY{p}{(}title \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Scatterplot Matrix\PYZdq{}}\PY{p}{,}
                          columns \PY{o}{=} \PY{l+m}{2}\PY{p}{,} 
                          points \PY{o}{=} \PY{k+kt}{list}\PY{p}{(}pch \PY{o}{=} super.sym\PY{o}{\PYZdl{}}pch\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{2}\PY{p}{]}\PY{p}{,}
                          col \PY{o}{=} super.sym\PY{o}{\PYZdl{}}\PY{k+kp}{col}\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{2}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                          text \PY{o}{=} \PY{k+kt}{list}\PY{p}{(}\PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Not Left\PYZdq{}}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{Left\PYZdq{}}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_66_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \textbar{}

TASK

\begin{longtable}[]{@{}l@{}}
\toprule
\begin{minipage}[t]{0.07\columnwidth}\raggedright\strut
Write code to visually explore the relationship between of the number of
projects and leaving. What is a possible explanation for any patterns
you see?\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}30}]:} \PY{c+c1}{\PYZsh{} Explore number of projects and left}
         
         \PY{c+c1}{\PYZsh{} Create multiple figure}
         par\PY{p}{(}mfrow\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}\PY{l+m}{3}\PY{p}{,}\PY{l+m}{2}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Plot properties}
         label1 \PY{o}{\PYZlt{}\PYZhy{}} \PY{l+s}{\PYZsq{}}\PY{l+s}{Number of Projects Completed\PYZsq{}}
         label2 \PY{o}{\PYZlt{}\PYZhy{}} \PY{l+s}{\PYZsq{}}\PY{l+s}{Left\PYZsq{}}
         xBreaks \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kt}{c}\PY{p}{(}\PY{k+kp}{seq}\PY{p}{(}\PY{l+m}{0}\PY{p}{,} \PY{l+m}{7}\PY{p}{,} length\PY{o}{=}\PY{l+m}{8}\PY{p}{)}\PY{p}{)}
         red \PY{o}{\PYZlt{}\PYZhy{}} rgb\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}
         blue \PY{o}{\PYZlt{}\PYZhy{}} rgb\PY{p}{(}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Table for mosaic}
         project\PYZus{}table \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{table}\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}number\PYZus{}project\PY{p}{,} HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} mosaic}
         mosaicplot\PY{p}{(}project\PYZus{}table\PY{p}{,} main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Mosaic Plot of Left by Projects\PYZdq{}}\PY{p}{,} xlab\PY{o}{=}label1\PY{p}{,} ylab\PY{o}{=}label2\PY{p}{,} las\PY{o}{=}\PY{l+m}{2}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} boxplot}
         boxplot\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}number\PYZus{}project\PY{o}{\PYZti{}}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{p}{,} xlab\PY{o}{=}label2\PY{p}{,} ylab\PY{o}{=}label1\PY{p}{,} main\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{Boxplot of Left by Projects\PYZsq{}}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} histogram \PYZhy{} frequency}
         hist\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}number\PYZus{}project\PY{p}{[}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{o}{==}\PY{l+m}{1}\PY{p}{]}\PY{p}{,} col\PY{o}{=}red\PY{p}{,} freq\PY{o}{=}\PY{k+kc}{TRUE}\PY{p}{,} breaks\PY{o}{=}xBreaks\PY{p}{,} ylim\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{4500}\PY{p}{)}\PY{p}{,}
             main\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{Histogram for Number of Projects by Left\PYZsq{}}\PY{p}{,} cex.main\PY{o}{=}\PY{l+m}{0.9}\PY{p}{,} xlab\PY{o}{=}label1\PY{p}{)}
         hist\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}number\PYZus{}project\PY{p}{[}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{o}{==}\PY{l+m}{0}\PY{p}{]}\PY{p}{,} add\PY{o}{=}\PY{k+kc}{TRUE}\PY{p}{,} col\PY{o}{=}blue\PY{p}{,} freq\PY{o}{=}\PY{k+kc}{TRUE}\PY{p}{,} breaks\PY{o}{=}xBreaks\PY{p}{,} cex.main\PY{o}{=}\PY{l+m}{0.9}\PY{p}{)}
         legend\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{topright\PYZsq{}}\PY{p}{,} \PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Left\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{Not Left\PYZsq{}}\PY{p}{)}\PY{p}{,} fill\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}red\PY{p}{,} blue\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} histogram \PYZhy{} density}
         hist\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}number\PYZus{}project\PY{p}{[}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{o}{==}\PY{l+m}{1}\PY{p}{]}\PY{p}{,} col\PY{o}{=}red\PY{p}{,} freq\PY{o}{=}\PY{k+kc}{FALSE}\PY{p}{,} breaks\PY{o}{=}xBreaks\PY{p}{,}
             main\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{Histogram for Number of Projects by Left\PYZsq{}}\PY{p}{,} cex.main\PY{o}{=}\PY{l+m}{0.9}\PY{p}{,} xlab\PY{o}{=}label1\PY{p}{)}
         hist\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}number\PYZus{}project\PY{p}{[}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{o}{==}\PY{l+m}{0}\PY{p}{]}\PY{p}{,} add\PY{o}{=}\PY{k+kc}{TRUE}\PY{p}{,} col\PY{o}{=}blue\PY{p}{,} freq\PY{o}{=}\PY{k+kc}{FALSE}\PY{p}{,} breaks\PY{o}{=}xBreaks\PY{p}{,} 
              cex.main\PY{o}{=}\PY{l+m}{0.9}\PY{p}{)}
         legend\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{topright\PYZsq{}}\PY{p}{,} \PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Left\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{Not Left\PYZsq{}}\PY{p}{)}\PY{p}{,} fill\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}red\PY{p}{,} blue\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Empircal Cumulative Distribution }
         E0 \PY{o}{\PYZlt{}\PYZhy{}} ecdf\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}number\PYZus{}project\PY{p}{[}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{o}{==}\PY{l+m}{0}\PY{p}{]}\PY{p}{)}
         E1 \PY{o}{\PYZlt{}\PYZhy{}} ecdf\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}number\PYZus{}project\PY{p}{[}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{o}{==}\PY{l+m}{1}\PY{p}{]}\PY{p}{)}
         plot\PY{p}{(}E0\PY{p}{,} col\PY{o}{=}blue\PY{p}{,} verticals\PY{o}{=}\PY{k+kc}{TRUE}\PY{p}{,} do.points\PY{o}{=}\PY{k+kc}{FALSE}\PY{p}{,} main\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{ECDF for Projects by Left\PYZsq{}}\PY{p}{,} xlab\PY{o}{=}label1\PY{p}{)}
         plot\PY{p}{(}E1\PY{p}{,} col\PY{o}{=}red\PY{p}{,} verticals\PY{o}{=}\PY{k+kc}{TRUE}\PY{p}{,} do.points\PY{o}{=}\PY{k+kc}{FALSE}\PY{p}{,} add\PY{o}{=}\PY{k+kc}{TRUE}\PY{p}{)}
         legend\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{bottomright\PYZsq{}}\PY{p}{,} \PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Not Left\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{Left\PYZsq{}}\PY{p}{)}\PY{p}{,}lwd\PY{o}{=}\PY{l+m}{1}\PY{p}{,} lty\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{1}\PY{p}{)}\PY{p}{,} col\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}blue\PY{p}{,}red\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Let\PYZsq{}s look at projects completed by time at company}
         plot\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}time\PYZus{}spend\PYZus{}company\PY{p}{[}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{o}{==}\PY{l+m}{0}\PY{p}{]}\PY{p}{,}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}number\PYZus{}project\PY{p}{[}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{o}{==}\PY{l+m}{0}\PY{p}{]}\PY{p}{,} 
              col\PY{o}{=}blue\PY{p}{,} main\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{Scatter plot for Time Spent at Company vs \PYZbs{}n Projects Completed\PYZsq{}}\PY{p}{,} cex.main\PY{o}{=}\PY{l+m}{0.8}\PY{p}{,}
             xlab\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{Time at Company (years)\PYZsq{}}\PY{p}{,} ylab\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{Number of Projects Completed\PYZsq{}}\PY{p}{,} ylim\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{8}\PY{p}{)}\PY{p}{,} xlim\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{10}\PY{p}{)}\PY{p}{)}
         points\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}time\PYZus{}spend\PYZus{}company\PY{p}{[}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{o}{==}\PY{l+m}{1}\PY{p}{]}\PY{p}{,}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}number\PYZus{}project\PY{p}{[}HR\PYZus{}comma\PYZus{}sep\PY{o}{\PYZdl{}}left\PY{o}{==}\PY{l+m}{1}\PY{p}{]}\PY{p}{,}
               col\PY{o}{=}red\PY{p}{,} pch\PY{o}{=}\PY{l+m}{2}\PY{p}{)}
         legend\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{topright\PYZsq{}}\PY{p}{,} \PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Not Left\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{Left\PYZsq{}}\PY{p}{)}\PY{p}{,} pch\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{2}\PY{p}{)}\PY{p}{,} col\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}blue\PY{p}{,}red\PY{p}{)}\PY{p}{,} cex\PY{o}{=}\PY{l+m}{0.5}\PY{p}{)}
         grid\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_68_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The plots suggest that there is a much greater chance of an employee
leaving if they haven't completed a third project. This is clear in the
mosaic and histograms where employees with 2-5 project completions make
up the bulk of those who are still at the company and employees with 2
project completions are the most likely to leave. Comparing number of
projects completed with time spent at company doesn't show any clear
relationship with staying or leaving and number of projects competed
over time.

Two projects may be enough for an employee to decide if they like the
work they are doing and whether they want to stay or leave, and in a
similar vein, it may be enough for the employer to decide whether they
want to keep the employee on or let them go.

    So far, we have been exploring the entire data set. Let us return to the
original question: \emph{Why are our best and most experienced employees
leaving prematurely?}

To get to grips with this, we need to identify which subset of employees
are "best" and "most experienced". Precisely what this means to any
particular person is ambiguous. When encountering ambiguity in the
problem, the process of resolving that ambiguity involves a two-way
dialogue with the \emph{problem poser}.

Broadly, one might imagine this subset to contain:

\begin{itemize}
\tightlist
\item
  Employees with high evaluations.
\item
  Employees the have been with the company for a while.
\end{itemize}

Additional criteria might be:

\begin{itemize}
\tightlist
\item
  Employees that work on a large number of projects.
\item
  Employees that work a lot.
\end{itemize}

For now, suppose that the "best" employees are those with an evaluation
of 0.8 or higher, and the "most experienced" employees are those that
have been with the company for 4 or more years.

\textbar{}

TASK

\begin{longtable}[]{@{}l@{}}
\toprule
\begin{minipage}[t]{0.07\columnwidth}\raggedright\strut
Create a subset of the "best" and "most experienced" employees by
appropriately filtering the entire data set.\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}31}]:} \PY{c+c1}{\PYZsh{} Get subset}
         best\PYZus{}and\PYZus{}exp \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{subset}\PY{p}{(}HR\PYZus{}comma\PYZus{}sep\PY{p}{,} last\PYZus{}evaluation\PY{o}{\PYZgt{}=}\PY{l+m}{0.8} \PY{o}{\PYZam{}} time\PYZus{}spend\PYZus{}company\PY{o}{\PYZgt{}=}\PY{l+m}{4}\PY{p}{)} \PY{c+c1}{\PYZsh{} Best and Most Experienced}
         
         \PY{c+c1}{\PYZsh{} Summary}
         \PY{k+kp}{summary}\PY{p}{(}best\PYZus{}and\PYZus{}exp\PY{p}{)}
         
         best\PYZus{}and\PYZus{}exp
\end{Verbatim}


    
    \begin{verbatim}
 satisfaction_level last_evaluation  number_project  average_montly_hours
 Min.   :0.0900     Min.   :0.8000   Min.   :2.000   Min.   : 96.0       
 1st Qu.:0.1100     1st Qu.:0.8500   1st Qu.:4.000   1st Qu.:210.0       
 Median :0.6500     Median :0.9000   Median :5.000   Median :245.0       
 Mean   :0.5323     Mean   :0.9019   Mean   :4.758   Mean   :234.6       
 3rd Qu.:0.8200     3rd Qu.:0.9500   3rd Qu.:6.000   3rd Qu.:267.0       
 Max.   :1.0000     Max.   :1.0000   Max.   :7.000   Max.   :310.0       
 time_spend_company Work_accident          left       promotion_last_5years
 Min.   : 4.00      Min.   :0.00000   Min.   :0.000   Min.   :0.00000      
 1st Qu.: 4.00      1st Qu.:0.00000   1st Qu.:0.000   1st Qu.:0.00000      
 Median : 5.00      Median :0.00000   Median :1.000   Median :0.00000      
 Mean   : 4.89      Mean   :0.09688   Mean   :0.581   Mean   :0.01541      
 3rd Qu.: 5.00      3rd Qu.:0.00000   3rd Qu.:1.000   3rd Qu.:0.00000      
 Max.   :10.00      Max.   :1.00000   Max.   :1.000   Max.   :1.00000      
    sales              salary         
 Length:2921        Length:2921       
 Class :character   Class :character  
 Mode  :character   Mode  :character  
                                      
                                      
                                      
    \end{verbatim}

    
    \begin{tabular}{r|llllllllll}
 satisfaction\_level & last\_evaluation & number\_project & average\_montly\_hours & time\_spend\_company & Work\_accident & left & promotion\_last\_5years & sales & salary\\
\hline
	 0.80        & 0.86        & 5           & 262         & 6           & 0           & 1           & 0           & sales       & medium     \\
	 0.11        & 0.88        & 7           & 272         & 4           & 0           & 1           & 0           & sales       & medium     \\
	 0.72        & 0.87        & 5           & 223         & 5           & 0           & 1           & 0           & sales       & low        \\
	 0.92        & 0.85        & 5           & 259         & 5           & 0           & 1           & 0           & sales       & low        \\
	 0.89        & 1.00        & 5           & 224         & 5           & 0           & 1           & 0           & sales       & low        \\
	 0.11        & 0.81        & 6           & 305         & 4           & 0           & 1           & 0           & sales       & low        \\
	 0.84        & 0.92        & 4           & 234         & 5           & 0           & 1           & 0           & sales       & low        \\
	 0.78        & 0.99        & 4           & 255         & 6           & 0           & 1           & 0           & sales       & low        \\
	 0.76        & 0.89        & 5           & 262         & 5           & 0           & 1           & 0           & sales       & low        \\
	 0.11        & 0.83        & 6           & 282         & 4           & 0           & 1           & 0           & sales       & low        \\
	 0.09        & 0.95        & 6           & 304         & 4           & 0           & 1           & 0           & sales       & low        \\
	 0.89        & 0.92        & 5           & 242         & 5           & 0           & 1           & 0           & sales       & low        \\
	 0.82        & 0.87        & 4           & 239         & 5           & 0           & 1           & 0           & sales       & low        \\
	 0.84        & 0.87        & 4           & 246         & 6           & 0           & 1           & 0           & hr          & low        \\
	 0.10        & 0.94        & 6           & 255         & 4           & 0           & 1           & 0           & technical   & low        \\
	 0.11        & 0.89        & 6           & 306         & 4           & 0           & 1           & 0           & technical   & low        \\
	 0.87        & 0.88        & 5           & 269         & 5           & 0           & 1           & 0           & technical   & low        \\
	 0.10        & 0.80        & 7           & 281         & 4           & 0           & 1           & 0           & technical   & low        \\
	 0.09        & 0.89        & 6           & 276         & 4           & 0           & 1           & 0           & technical   & low        \\
	 0.10        & 0.92        & 7           & 307         & 4           & 0           & 1           & 0           & support     & low        \\
	 0.11        & 0.94        & 7           & 255         & 4           & 0           & 1           & 0           & support     & low        \\
	 0.10        & 0.81        & 6           & 309         & 4           & 0           & 1           & 0           & technical   & low        \\
	 0.85        & 1.00        & 4           & 225         & 5           & 0           & 1           & 0           & technical   & low        \\
	 0.85        & 0.91        & 5           & 226         & 5           & 0           & 1           & 0           & management  & medium     \\
	 0.11        & 0.93        & 7           & 308         & 4           & 0           & 1           & 0           & IT          & medium     \\
	 0.10        & 0.95        & 6           & 244         & 5           & 0           & 1           & 0           & IT          & medium     \\
	 0.11        & 0.94        & 6           & 286         & 4           & 0           & 1           & 0           & IT          & medium     \\
	 0.90          & 0.98          & 4             & 264           & 6             & 0             & 1             & 0             & product\_mng & medium       \\
	 0.76          & 0.86          & 5             & 223           & 5             & 1             & 1             & 0             & product\_mng & medium       \\
	 0.09          & 0.87          & 7             & 295           & 4             & 0             & 1             & 0             & product\_mng & low          \\
	 ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮\\
	 0.11        & 0.89        & 6           & 268         & 4           & 0           & 1           & 0           & IT          & medium     \\
	 0.87        & 0.88        & 5           & 240         & 5           & 0           & 1           & 0           & IT          & medium     \\
	 0.10        & 0.94        & 7           & 264         & 4           & 0           & 1           & 0           & IT          & medium     \\
	 0.76          & 0.98          & 5             & 217           & 6             & 0             & 1             & 0             & product\_mng & medium       \\
	 0.90        & 0.92        & 4           & 271         & 5           & 0           & 1           & 0           & IT          & medium     \\
	 0.85        & 0.87        & 4           & 273         & 5           & 0           & 1           & 0           & RandD       & medium     \\
	 0.81        & 0.92        & 5           & 239         & 5           & 0           & 1           & 0           & RandD       & medium     \\
	 0.83        & 0.85        & 5           & 237         & 5           & 0           & 1           & 0           & marketing   & medium     \\
	 0.10        & 0.84        & 5           & 303         & 5           & 0           & 1           & 0           & accounting  & medium     \\
	 0.91        & 0.98        & 4           & 242         & 6           & 0           & 1           & 0           & support     & low        \\
	 0.72        & 0.94        & 4           & 258         & 5           & 0           & 1           & 0           & sales       & low        \\
	 0.82        & 0.94        & 5           & 236         & 5           & 0           & 1           & 0           & sales       & low        \\
	 0.59        & 1.00        & 2           & 155         & 5           & 0           & 1           & 0           & sales       & low        \\
	 0.82        & 0.86        & 5           & 257         & 5           & 0           & 1           & 0           & sales       & low        \\
	 0.09        & 0.95        & 6           & 271         & 4           & 0           & 1           & 0           & sales       & low        \\
	 0.10        & 0.97        & 6           & 280         & 4           & 0           & 1           & 0           & sales       & low        \\
	 0.83        & 0.81        & 5           & 219         & 5           & 0           & 1           & 0           & sales       & low        \\
	 0.78        & 0.93        & 4           & 225         & 5           & 0           & 1           & 0           & sales       & medium     \\
	 0.11        & 0.97        & 6           & 310         & 4           & 0           & 1           & 0           & accounting  & medium     \\
	 0.81        & 0.85        & 4           & 251         & 6           & 0           & 1           & 0           & hr          & medium     \\
	 0.09        & 0.93        & 6           & 296         & 4           & 0           & 1           & 0           & technical   & medium     \\
	 0.76        & 0.89        & 5           & 238         & 5           & 0           & 1           & 0           & technical   & high       \\
	 0.73        & 0.93        & 5           & 162         & 4           & 0           & 1           & 0           & technical   & low        \\
	 0.72        & 0.84        & 5           & 257         & 5           & 0           & 1           & 0           & technical   & medium     \\
	 0.91        & 0.99        & 5           & 254         & 5           & 0           & 1           & 0           & technical   & medium     \\
	 0.85        & 0.85        & 4           & 247         & 6           & 0           & 1           & 0           & technical   & low        \\
	 0.89        & 0.88        & 5           & 228         & 5           & 1           & 1           & 0           & support     & low        \\
	 0.09        & 0.81        & 6           & 257         & 4           & 0           & 1           & 0           & support     & low        \\
	 0.76        & 0.83        & 6           & 293         & 6           & 0           & 1           & 0           & support     & low        \\
	 0.11        & 0.96        & 6           & 280         & 4           & 0           & 1           & 0           & support     & low        \\
\end{tabular}


    
    \textbar{}

TASK

\begin{longtable}[]{@{}l@{}}
\toprule
\begin{minipage}[t]{0.07\columnwidth}\raggedright\strut
Perform EDA on the subset of "best" and "most experienced" employees you
just created. What is a possible explanation for any patterns you
see?\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}32}]:} \PY{c+c1}{\PYZsh{} Make scatterplot matrix}
         \PY{k+kn}{library}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{lattice\PYZsq{}}\PY{p}{)}
         key\PYZus{}columns \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{subset}\PY{p}{(}\PY{k+kp}{as.data.frame}\PY{p}{(}best\PYZus{}and\PYZus{}exp\PY{p}{)}\PY{p}{,} select\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{satisfaction\PYZus{}level\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{number\PYZus{}project\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{average\PYZus{}montly\PYZus{}hours\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{time\PYZus{}spend\PYZus{}company\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{left\PYZsq{}}\PY{p}{)}\PY{p}{)}
         super.sym \PY{o}{\PYZlt{}\PYZhy{}} trellis.par.get\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{superpose.symbol\PYZdq{}}\PY{p}{)} \PY{c+c1}{\PYZsh{}Get Symbol Plotting Information}
         splom\PY{p}{(}\PY{o}{\PYZti{}}key\PYZus{}columns\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{4}\PY{p}{]}\PY{p}{,}groups\PY{o}{=}left\PY{p}{,}data \PY{o}{=} key\PYZus{}columns\PY{p}{,}varnames\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Satisfaction \PYZbs{}nLevel\PYZdq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{Number of \PYZbs{}nProjects\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{Average \PYZbs{}nMonthly \PYZbs{}nHours\PYZdq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{Time \PYZbs{}nSpent at \PYZbs{}nCompany\PYZsq{}}\PY{p}{)}\PY{p}{,}
               key \PY{o}{=} \PY{k+kt}{list}\PY{p}{(}title \PY{o}{=} \PY{l+s}{\PYZdq{}}\PY{l+s}{Scatterplot Matrix\PYZdq{}}\PY{p}{,}
                          columns \PY{o}{=} \PY{l+m}{2}\PY{p}{,} 
                          points \PY{o}{=} \PY{k+kt}{list}\PY{p}{(}pch \PY{o}{=} super.sym\PY{o}{\PYZdl{}}pch\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{2}\PY{p}{]}\PY{p}{,}
                          col \PY{o}{=} super.sym\PY{o}{\PYZdl{}}\PY{k+kp}{col}\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{2}\PY{p}{]}\PY{p}{)}\PY{p}{,}
                          text \PY{o}{=} \PY{k+kt}{list}\PY{p}{(}\PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Not Left\PYZdq{}}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{Left\PYZdq{}}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Left compared with promotion}
         comp\PYZus{}table \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{table}\PY{p}{(}best\PYZus{}and\PYZus{}exp\PY{o}{\PYZdl{}}promotion\PYZus{}last\PYZus{}5years\PY{p}{,} best\PYZus{}and\PYZus{}exp\PY{o}{\PYZdl{}}left\PY{p}{)}
         mosaicplot\PY{p}{(}comp\PYZus{}table\PY{p}{,} main\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{Mosaic Plot of Left and if Received Promotion\PYZsq{}}\PY{p}{,} xlab\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{Promotion Last 5 years\PYZsq{}}\PY{p}{,} ylab\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{Left\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_73_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_73_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The scatter plot matrix shows some interesting trends for the best and
most experienced employees that have left. Things we can can conclude
are these employees had generally spent 4-6 years at the company,
completed 4-7 projects, worked longer hours, 200+ a month up to 300
hours, and had either a high satisfaction or a very low satisfaction
with more tending to have a high satisfaction.

I think that the lower satisfaction, best and most experienced
employees, have left because of this dissatisfaction level either. It is
more difficult to say whether this is directed towards the company or
unsatisfaction with their line of work. The higher satisfaction, best
and most experience employeers, that left may have sort better job
offers or have been 'poached' by other compaines because of their
talent.

The mosaic comparing left to promotion recived in last 5 years shows
hardly any promotions within the company for best and most experienced
employeers. It doesn't show any clear pattern for no promotion to
leaving the company though.

    \section{Exploring Unusual Data}\label{exploring-unusual-data}

Codifying which observations are "unusual" goes hand-in-hand with the
our statistical model for data; in particular the assumptions we make
about the distribution of the residuals.

A common assumption in statistical analyses is that the residuals follow
a \emph{normal distribution}.

Let's simulate 200 normally distributed observations, synthesising
heights (in cm) of adult men, plot a histogram, and display standard
summary statistics.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}33}]:} height\PYZus{}data\PY{o}{\PYZlt{}\PYZhy{}}rnorm\PY{p}{(}\PY{l+m}{200}\PY{p}{,} mean \PY{o}{=} \PY{l+m}{174.46}\PY{p}{,} sd \PY{o}{=} \PY{l+m}{7.15}\PY{p}{)} 
         hist\PY{p}{(}height\PYZus{}data\PY{p}{,}col\PY{o}{=}rgb\PY{p}{(}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Histogram for Synthetic Height Data\PYZdq{}}\PY{p}{,} xlab\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Height (cm)\PYZdq{}}\PY{p}{)}
         \PY{k+kp}{summary}\PY{p}{(}height\PYZus{}data\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  150.3   168.7   173.9   174.1   179.3   192.9 
    \end{verbatim}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_76_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    You will notice the symmetric unimodal shape of the histogram. Re-run
the code above several times and observe how the distribution of the
data retains these characteristics.

Next, we will create a copy of the data, artificially convert the first
10 observations from units of cm to units of inches, plot another
histogram, and display standard summary statistics.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}34}]:} height\PYZus{}data\PYZus{}out\PY{o}{\PYZlt{}\PYZhy{}}height\PYZus{}data
         height\PYZus{}data\PYZus{}out\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{10}\PY{p}{]}\PY{o}{\PYZlt{}\PYZhy{}}height\PYZus{}data\PYZus{}out\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{10}\PY{p}{]}\PY{o}{*}\PY{l+m}{0.3937007874}
         hist\PY{p}{(}height\PYZus{}data\PYZus{}out\PY{p}{,}col\PY{o}{=}rgb\PY{p}{(}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{0}\PY{p}{,}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Histogram for Synthetic Height Data (with Unusual Obs.)\PYZdq{}}\PY{p}{,} xlab\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Height (cm)\PYZdq{}}\PY{p}{)}
         \PY{k+kp}{summary}\PY{p}{(}height\PYZus{}data\PYZus{}out\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  62.01  168.08  173.22  168.90  179.03  192.88 
    \end{verbatim}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_78_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    For this artifical data, it is immediately clear from the histogram that
there are unusual observations, widely separated from the bulk.

Despite both being measures of central tendency, you will notice that
the \texttt{mean} values have changed noticably, but the \texttt{median}
values have barely moved. This is because the \texttt{mean} is a
sensitive statistic (and consequently is greatly impacted by unusual
observations; influential observations in this case). On the other hand
the \texttt{median} is a rather insensitive (a \emph{robust statistic},
and is hardly impacted by unusual observations).

Using simple robust measures of central tendency (median) and
variability {[}interquartile range (IQR){]}, the boxplot visually flags
observations that lie outside
\(\textrm{Median} \pm 1.5 \times \textrm{IQR}\). These are the
observations that may depart from the underlying assumption of
normality; the outliers.

Let's create and store a boxplot of the height data with unusual
observations.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}35}]:} hdo\PYZus{}box\PY{o}{\PYZlt{}\PYZhy{}}boxplot\PY{p}{(}height\PYZus{}data\PYZus{}out\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Boxplot of Synthetic Height Data (with Unusual Obs.)\PYZdq{}}\PY{p}{,} ylab\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Height (cm)\PYZdq{}}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_80_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    All of our artifical unusual observations are clearly labelled as
outliers.

We can extract them from the stored boxplot as follows.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}36}]:} hdo\PYZus{}box\PY{o}{\PYZdl{}}out
\end{Verbatim}


    \begin{enumerate*}
\item 67.0084794886832
\item 69.3948343854613
\item 66.2166257454972
\item 64.4161871071776
\item 64.4136414275184
\item 72.6822707689947
\item 68.5605716494176
\item 68.0587956686257
\item 62.0101930645932
\item 68.5566987754323
\item 150.276999239915
\end{enumerate*}


    
    In this instance, after examining the outliers and the context of the
data set, it is clear that the reason for these outliers is a simple
unit change.

    \section{Part 3 Data Imputation}\label{part-3-data-imputation}

Let's load up Karl Pearsons' data on the heights (in inches) of fathers
and their sons, produce a scatter plot of the data, and display standard
summary statistics.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}37}]:} \PY{k+kn}{library}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{readr\PYZdq{}}\PY{p}{)}
         fheight\PY{o}{\PYZlt{}\PYZhy{}}read\PYZus{}csv\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{./PearsonFather.csv\PYZdq{}}\PY{p}{,}col\PYZus{}names\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{fheight\PYZdq{}}\PY{p}{,}col\PYZus{}types\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{d\PYZdq{}}\PY{p}{)}
         sheight\PY{o}{\PYZlt{}\PYZhy{}}read\PYZus{}csv\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{./PearsonSon.csv\PYZdq{}}\PY{p}{,}col\PYZus{}names\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{sheight\PYZdq{}}\PY{p}{,}col\PYZus{}types\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{d\PYZdq{}}\PY{p}{)}
         fs\PYZus{}height\PY{o}{\PYZlt{}\PYZhy{}}\PY{k+kt}{data.frame}\PY{p}{(}fheight\PY{p}{,}sheight\PY{p}{)}
         plot\PY{p}{(}fs\PYZus{}height\PY{o}{\PYZdl{}}fheight\PY{p}{,}fs\PYZus{}height\PY{o}{\PYZdl{}}sheight\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Father and Son Heights\PYZdq{}}\PY{p}{,} xlab\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Father Heights (in)\PYZdq{}}\PY{p}{,}ylab\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Son Heights (in)\PYZdq{}}\PY{p}{,}col\PY{o}{=}rgb\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.3}\PY{p}{,}\PY{l+m}{0.1}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{)}
         \PY{k+kp}{summary}\PY{p}{(}fs\PYZus{}height\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
    fheight         sheight     
 Min.   :59.00   Min.   :58.50  
 1st Qu.:65.80   1st Qu.:66.90  
 Median :67.80   Median :68.60  
 Mean   :67.69   Mean   :68.68  
 3rd Qu.:69.60   3rd Qu.:70.50  
 Max.   :75.40   Max.   :78.40  
    \end{verbatim}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_85_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Now, let's create a version of the data set with both father and son
heights missing completely at random (MCAR), plot the data, and display
summary statistics.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}38}]:} fheight\PYZus{}MCAR\PY{o}{\PYZlt{}\PYZhy{}}fheight
         fheight\PYZus{}MCAR\PY{p}{[}rbinom\PY{p}{(}\PY{k+kp}{nrow}\PY{p}{(}fheight\PY{p}{)}\PY{p}{,}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.1}\PY{p}{)}\PY{o}{==}\PY{l+m}{1}\PY{p}{,}\PY{p}{]}\PY{o}{\PYZlt{}\PYZhy{}}\PY{k+kc}{NA}
         sheight\PYZus{}MCAR\PY{o}{\PYZlt{}\PYZhy{}}sheight
         sheight\PYZus{}MCAR\PY{p}{[}rbinom\PY{p}{(}\PY{k+kp}{nrow}\PY{p}{(}sheight\PY{p}{)}\PY{p}{,}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.1}\PY{p}{)}\PY{o}{==}\PY{l+m}{1}\PY{p}{,}\PY{p}{]}\PY{o}{\PYZlt{}\PYZhy{}}\PY{k+kc}{NA}
         fs\PYZus{}height\PYZus{}MCAR\PY{o}{\PYZlt{}\PYZhy{}}\PY{k+kt}{data.frame}\PY{p}{(}fheight\PYZus{}MCAR\PY{p}{,}sheight\PYZus{}MCAR\PY{p}{)}
         
         plot\PY{p}{(}fs\PYZus{}height\PYZus{}MCAR\PY{o}{\PYZdl{}}fheight\PY{p}{,}fs\PYZus{}height\PYZus{}MCAR\PY{o}{\PYZdl{}}sheight\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Father and Son Heights (MCAR)\PYZdq{}}\PY{p}{,} xlab\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Father Heights (in)\PYZdq{}}\PY{p}{,}ylab\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Son Heights (in)\PYZdq{}}\PY{p}{,}col\PY{o}{=}rgb\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.3}\PY{p}{,}\PY{l+m}{0.1}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{)}
         \PY{k+kp}{summary}\PY{p}{(}fs\PYZus{}height\PYZus{}MCAR\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}
    fheight         sheight     
 Min.   :59.00   Min.   :58.50  
 1st Qu.:65.80   1st Qu.:66.95  
 Median :67.80   Median :68.60  
 Mean   :67.72   Mean   :68.69  
 3rd Qu.:69.60   3rd Qu.:70.40  
 Max.   :75.40   Max.   :78.40  
 NA's   :96      NA's   :107    
    \end{verbatim}

    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_87_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Now lets examine the effect of some simple imputation strategies, using
the \texttt{mice} package.

First up is mean imputation (\textbf{not recommended ever in practice})

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}39}]:} \PY{k+kn}{library}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{mice\PYZdq{}}\PY{p}{)}
         imp\PYZus{}mean\PY{o}{\PYZlt{}\PYZhy{}}mice\PY{p}{(}fs\PYZus{}height\PYZus{}MCAR\PY{p}{,}method\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{mean\PYZdq{}}\PY{p}{,}m\PY{o}{=}\PY{l+m}{1}\PY{p}{,}maxit\PY{o}{=}\PY{l+m}{1}\PY{p}{)}
         fsh\PYZus{}mean\PYZus{}imp\PY{o}{\PYZlt{}\PYZhy{}}complete\PY{p}{(}imp\PYZus{}mean\PY{p}{,}\PY{l+m}{1}\PY{p}{)}
         fsh\PYZus{}mean\PYZus{}imp\PY{o}{\PYZdl{}}mis\PY{o}{\PYZlt{}\PYZhy{}}\PY{o}{!}complete.cases\PY{p}{(}fs\PYZus{}height\PYZus{}MCAR\PY{p}{)}
         plot\PY{p}{(}fsh\PYZus{}mean\PYZus{}imp\PY{o}{\PYZdl{}}fheight\PY{p}{[}fsh\PYZus{}mean\PYZus{}imp\PY{o}{\PYZdl{}}mis\PY{o}{==}\PY{k+kc}{FALSE}\PY{p}{]}\PY{p}{,}fsh\PYZus{}mean\PYZus{}imp\PY{o}{\PYZdl{}}sheight\PY{p}{[}fsh\PYZus{}mean\PYZus{}imp\PY{o}{\PYZdl{}}mis\PY{o}{==}\PY{k+kc}{FALSE}\PY{p}{]}\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Father and Son Heights (MCAR)\PYZdq{}}\PY{p}{,} xlab\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Father Heights (in)\PYZdq{}}\PY{p}{,}ylab\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Son Heights (in)\PYZdq{}}\PY{p}{,}col\PY{o}{=}rgb\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.3}\PY{p}{,}\PY{l+m}{0.1}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{)}
         points\PY{p}{(}fsh\PYZus{}mean\PYZus{}imp\PY{o}{\PYZdl{}}fheight\PY{p}{[}fsh\PYZus{}mean\PYZus{}imp\PY{o}{\PYZdl{}}mis\PY{o}{==}\PY{k+kc}{TRUE}\PY{p}{]}\PY{p}{,}fsh\PYZus{}mean\PYZus{}imp\PY{o}{\PYZdl{}}sheight\PY{p}{[}fsh\PYZus{}mean\PYZus{}imp\PY{o}{\PYZdl{}}mis\PY{o}{==}\PY{k+kc}{TRUE}\PY{p}{]}\PY{p}{,}col\PY{o}{=}rgb\PY{p}{(}\PY{l+m}{0.1}\PY{p}{,}\PY{l+m}{0.7}\PY{p}{,}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{)}
         legend\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{bottomright\PYZdq{}}\PY{p}{,} \PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Complete Cases\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{Imputed Cases\PYZdq{}}\PY{p}{)}\PY{p}{,}pch\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{1}\PY{p}{)}\PY{p}{,}col\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}rgb\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.3}\PY{p}{,}\PY{l+m}{0.1}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{,} rgb\PY{p}{(}\PY{l+m}{0.1}\PY{p}{,}\PY{l+m}{0.7}\PY{p}{,}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

Attaching package: ‘mice’

The following object is masked from ‘package:tidyr’:

    complete

The following objects are masked from ‘package:base’:

    cbind, rbind


    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]

 iter imp variable
  1   1  fheight  sheight

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_89_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    The distortion to the data is clearly observable in the plot above. This
distortion is reflected in its effect on the standard deviations of the
variables, as can be seen as follows.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}40}]:} \PY{k+kp}{as.data.frame}\PY{p}{(}\PY{k+kp}{lapply}\PY{p}{(}fs\PYZus{}height\PY{p}{,}sd\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} Source Data Standard Deviations}
         \PY{k+kp}{as.data.frame}\PY{p}{(}\PY{k+kp}{lapply}\PY{p}{(}fs\PYZus{}height\PYZus{}MCAR\PY{p}{,}sd\PY{p}{,}na.rm\PY{o}{=}\PY{k+kc}{TRUE}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} Complete\PYZhy{}case MCAR Data Standard Deviations}
         \PY{k+kp}{as.data.frame}\PY{p}{(}\PY{k+kp}{lapply}\PY{p}{(}fsh\PYZus{}mean\PYZus{}imp\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{2}\PY{p}{]}\PY{p}{,}sd\PY{p}{,}na.rm\PY{o}{=}\PY{k+kc}{TRUE}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} Mean\PYZhy{}imputed MCAR Data Standard Deviations}
\end{Verbatim}


    \begin{tabular}{r|ll}
 fheight & sheight\\
\hline
	 2.745827 & 2.816194\\
\end{tabular}


    
    \begin{tabular}{r|ll}
 fheight & sheight\\
\hline
	 2.76229  & 2.791571\\
\end{tabular}


    
    \begin{tabular}{r|ll}
 fheight & sheight\\
\hline
	 2.636307 & 2.649273\\
\end{tabular}


    
    With a small change in commands, we can perform linear regression
imputation (using \texttt{method="norm.nob"}).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}41}]:} imp\PYZus{}linreg\PY{o}{\PYZlt{}\PYZhy{}}mice\PY{p}{(}fs\PYZus{}height\PYZus{}MCAR\PY{p}{,}method\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{norm.nob\PYZdq{}}\PY{p}{,}m\PY{o}{=}\PY{l+m}{1}\PY{p}{)}
         fsh\PYZus{}linreg\PYZus{}imp\PY{o}{\PYZlt{}\PYZhy{}}complete\PY{p}{(}imp\PYZus{}linreg\PY{p}{,}\PY{l+m}{1}\PY{p}{)}
         fsh\PYZus{}linreg\PYZus{}imp\PY{o}{\PYZdl{}}mis\PY{o}{\PYZlt{}\PYZhy{}}\PY{o}{!}complete.cases\PY{p}{(}fs\PYZus{}height\PYZus{}MCAR\PY{p}{)}
         plot\PY{p}{(}fsh\PYZus{}linreg\PYZus{}imp\PY{o}{\PYZdl{}}fheight\PY{p}{[}fsh\PYZus{}linreg\PYZus{}imp\PY{o}{\PYZdl{}}mis\PY{o}{==}\PY{k+kc}{FALSE}\PY{p}{]}\PY{p}{,}fsh\PYZus{}linreg\PYZus{}imp\PY{o}{\PYZdl{}}sheight\PY{p}{[}fsh\PYZus{}linreg\PYZus{}imp\PY{o}{\PYZdl{}}mis\PY{o}{==}\PY{k+kc}{FALSE}\PY{p}{]}\PY{p}{,}main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Father and Son Heights (MCAR)\PYZdq{}}\PY{p}{,} xlab\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Father Heights (in)\PYZdq{}}\PY{p}{,}ylab\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Son Heights (in)\PYZdq{}}\PY{p}{,}col\PY{o}{=}rgb\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.3}\PY{p}{,}\PY{l+m}{0.1}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{)}
         points\PY{p}{(}fsh\PYZus{}linreg\PYZus{}imp\PY{o}{\PYZdl{}}fheight\PY{p}{[}fsh\PYZus{}linreg\PYZus{}imp\PY{o}{\PYZdl{}}mis\PY{o}{==}\PY{k+kc}{TRUE}\PY{p}{]}\PY{p}{,}fsh\PYZus{}linreg\PYZus{}imp\PY{o}{\PYZdl{}}sheight\PY{p}{[}fsh\PYZus{}linreg\PYZus{}imp\PY{o}{\PYZdl{}}mis\PY{o}{==}\PY{k+kc}{TRUE}\PY{p}{]}\PY{p}{,}col\PY{o}{=}rgb\PY{p}{(}\PY{l+m}{0.1}\PY{p}{,}\PY{l+m}{0.7}\PY{p}{,}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{)}
         legend\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{bottomright\PYZdq{}}\PY{p}{,} \PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{Complete Cases\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{Imputed Cases\PYZdq{}}\PY{p}{)}\PY{p}{,}pch\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{1}\PY{p}{)}\PY{p}{,}col\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}rgb\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.3}\PY{p}{,}\PY{l+m}{0.1}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{,} rgb\PY{p}{(}\PY{l+m}{0.1}\PY{p}{,}\PY{l+m}{0.7}\PY{p}{,}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

 iter imp variable
  1   1  fheight  sheight
  2   1  fheight  sheight
  3   1  fheight  sheight
  4   1  fheight  sheight
  5   1  fheight  sheight

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_93_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Multiple imputation is requested by adjusting the \texttt{m=} argument;
for example \texttt{m=10} will carry out the imputation process 10
times.

As a final exercise, we will fit a linear regression model to (1) the
original data; (2) the complete-case data; (3) data multiply-imputated
with linear regression;

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}42}]:} fit\PYZus{}all\PY{o}{\PYZlt{}\PYZhy{}}\PY{k+kp}{with}\PY{p}{(}fs\PYZus{}height\PY{p}{,}lm\PY{p}{(}sheight\PY{o}{\PYZti{}}fheight\PY{p}{)}\PY{p}{)}
         
         fit\PYZus{}MCAR\PY{o}{\PYZlt{}\PYZhy{}}\PY{k+kp}{with}\PY{p}{(}fs\PYZus{}height\PYZus{}MCAR\PY{p}{,}lm\PY{p}{(}sheight\PY{o}{\PYZti{}}fheight\PY{p}{)}\PY{p}{)}
         
         mimp\PYZus{}linreg\PY{o}{\PYZlt{}\PYZhy{}}mice\PY{p}{(}fs\PYZus{}height\PYZus{}MCAR\PY{p}{,}method\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{norm.nob\PYZdq{}}\PY{p}{,}m\PY{o}{=}\PY{l+m}{10}\PY{p}{)}
         fit\PYZus{}linreg\PYZus{}mimp\PY{o}{\PYZlt{}\PYZhy{}}\PY{k+kp}{with}\PY{p}{(}mimp\PYZus{}linreg\PY{p}{,}lm\PY{p}{(}sheight\PY{o}{\PYZti{}}fheight\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

 iter imp variable
  1   1  fheight  sheight
  1   2  fheight  sheight
  1   3  fheight  sheight
  1   4  fheight  sheight
  1   5  fheight  sheight
  1   6  fheight  sheight
  1   7  fheight  sheight
  1   8  fheight  sheight
  1   9  fheight  sheight
  1   10  fheight  sheight
  2   1  fheight  sheight
  2   2  fheight  sheight
  2   3  fheight  sheight
  2   4  fheight  sheight
  2   5  fheight  sheight
  2   6  fheight  sheight
  2   7  fheight  sheight
  2   8  fheight  sheight
  2   9  fheight  sheight
  2   10  fheight  sheight
  3   1  fheight  sheight
  3   2  fheight  sheight
  3   3  fheight  sheight
  3   4  fheight  sheight
  3   5  fheight  sheight
  3   6  fheight  sheight
  3   7  fheight  sheight
  3   8  fheight  sheight
  3   9  fheight  sheight
  3   10  fheight  sheight
  4   1  fheight  sheight
  4   2  fheight  sheight
  4   3  fheight  sheight
  4   4  fheight  sheight
  4   5  fheight  sheight
  4   6  fheight  sheight
  4   7  fheight  sheight
  4   8  fheight  sheight
  4   9  fheight  sheight
  4   10  fheight  sheight
  5   1  fheight  sheight
  5   2  fheight  sheight
  5   3  fheight  sheight
  5   4  fheight  sheight
  5   5  fheight  sheight
  5   6  fheight  sheight
  5   7  fheight  sheight
  5   8  fheight  sheight
  5   9  fheight  sheight
  5   10  fheight  sheight

    \end{Verbatim}

    The linear regression fits found using each approach are summarised as
follows.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}43}]:} \PY{k+kp}{summary}\PY{p}{(}fit\PYZus{}all\PY{p}{)}
         \PY{k+kp}{summary}\PY{p}{(}fit\PYZus{}MCAR\PY{p}{)}
         \PY{k+kp}{summary}\PY{p}{(}pool\PY{p}{(}fit\PYZus{}linreg\PYZus{}mimp\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    
    \begin{verbatim}

Call:
lm(formula = sheight ~ fheight)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.8910 -1.5361 -0.0092  1.6359  8.9894 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 33.89280    1.83289   18.49   <2e-16 ***
fheight      0.51401    0.02706   19.00   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.438 on 1076 degrees of freedom
Multiple R-squared:  0.2512,	Adjusted R-squared:  0.2505 
F-statistic: 360.9 on 1 and 1076 DF,  p-value: < 2.2e-16

    \end{verbatim}

    
    
    \begin{verbatim}

Call:
lm(formula = sheight ~ fheight)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.8540 -1.5438 -0.1168  1.6341  8.3255 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 34.22636    2.01871   16.95   <2e-16 ***
fheight      0.50853    0.02977   17.08   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.441 on 883 degrees of freedom
  (193 observations deleted due to missingness)
Multiple R-squared:  0.2484,	Adjusted R-squared:  0.2475 
F-statistic: 291.7 on 1 and 883 DF,  p-value: < 2.2e-16

    \end{verbatim}

    
    \begin{tabular}{r|lllll}
  & estimate & std.error & statistic & df & p.value\\
\hline
	(Intercept) & 34.6889825 & 1.97259014 & 17.58550   & 291.8529   & 0         \\
	fheight &  0.5017993 & 0.02914711 & 17.21609   & 284.5461   & 0         \\
\end{tabular}


    
    Other imputation methods include simple random imputation from observed
data (\texttt{method="sample"}).

\textbar{}

TASK

\begin{longtable}[]{@{}l@{}}
\toprule
\begin{minipage}[t]{0.07\columnwidth}\raggedright\strut
Create and explore a single data set completed using simple random
imputation. Comment on how the data imputed using this method compares
to data imputed using linear regression.\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}44}]:} \PY{c+c1}{\PYZsh{} Imputation by random sampling}
         imp\PYZus{}random\PY{o}{\PYZlt{}\PYZhy{}}mice\PY{p}{(}fs\PYZus{}height\PYZus{}MCAR\PY{p}{,} method\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{sample\PYZsq{}}\PY{p}{,} m\PY{o}{=}\PY{l+m}{1}\PY{p}{)}
         fsh\PYZus{}random\PY{o}{\PYZlt{}\PYZhy{}}complete\PY{p}{(}imp\PYZus{}random\PY{p}{,}\PY{l+m}{1}\PY{p}{)} \PY{c+c1}{\PYZsh{} grab imputed values}
         fsh\PYZus{}random\PY{o}{\PYZdl{}}mis\PY{o}{\PYZlt{}\PYZhy{}}\PY{o}{!}complete.cases\PY{p}{(}fs\PYZus{}height\PYZus{}MCAR\PY{p}{)} \PY{c+c1}{\PYZsh{} create binary flag}
         
         \PY{c+c1}{\PYZsh{} plot colours}
         orange \PY{o}{=} col\PY{o}{=}rgb\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.3}\PY{p}{,}\PY{l+m}{0.1}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}
         blue \PY{o}{=} rgb\PY{p}{(}\PY{l+m}{0.1}\PY{p}{,}\PY{l+m}{0.7}\PY{p}{,}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{0.5}\PY{p}{)}
         
         \PY{c+c1}{\PYZsh{} Plot data}
         plot\PY{p}{(}fsh\PYZus{}random\PY{o}{\PYZdl{}}fheight\PY{p}{[}fsh\PYZus{}random\PY{o}{\PYZdl{}}mis\PY{o}{==}\PY{k+kc}{FALSE}\PY{p}{]}\PY{p}{,} fsh\PYZus{}random\PY{o}{\PYZdl{}}sheight\PY{p}{[}fsh\PYZus{}random\PY{o}{\PYZdl{}}mis\PY{o}{==}\PY{k+kc}{FALSE}\PY{p}{]}\PY{p}{,} col\PY{o}{=}orange\PY{p}{,}
             main\PY{o}{=}\PY{l+s}{\PYZdq{}}\PY{l+s}{Father and Son Heights (MCAR)\PYZdq{}}\PY{p}{,} xlab\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{Father\PYZsq{}}\PY{p}{,} ylab\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{Son\PYZsq{}}\PY{p}{)}
         points\PY{p}{(}fsh\PYZus{}random\PY{o}{\PYZdl{}}fheight\PY{p}{[}fsh\PYZus{}random\PY{o}{\PYZdl{}}mis\PY{o}{==}\PY{k+kc}{TRUE}\PY{p}{]}\PY{p}{,} fsh\PYZus{}random\PY{o}{\PYZdl{}}sheight\PY{p}{[}fsh\PYZus{}random\PY{o}{\PYZdl{}}mis\PY{o}{==}\PY{k+kc}{TRUE}\PY{p}{]}\PY{p}{,} col\PY{o}{=}blue\PY{p}{)}
         points\PY{p}{(}fsh\PYZus{}linreg\PYZus{}imp\PY{o}{\PYZdl{}}fheight\PY{p}{[}fsh\PYZus{}linreg\PYZus{}imp\PY{o}{\PYZdl{}}mis\PY{o}{==}\PY{k+kc}{TRUE}\PY{p}{]}\PY{p}{,}fsh\PYZus{}linreg\PYZus{}imp\PY{o}{\PYZdl{}}sheight\PY{p}{[}fsh\PYZus{}linreg\PYZus{}imp\PY{o}{\PYZdl{}}mis\PY{o}{==}\PY{k+kc}{TRUE}\PY{p}{]}\PY{p}{,} col\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{green\PYZsq{}}\PY{p}{)}
         legend\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{bottomright\PYZsq{}}\PY{p}{,} \PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{Complete Cases\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{Random Imputed Cases\PYZsq{}}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{Linear Regression Imputed Cases\PYZsq{}}\PY{p}{)}\PY{p}{,} 
                pch\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{1}\PY{p}{,}\PY{l+m}{1}\PY{p}{)}\PY{p}{,} col\PY{o}{=}\PY{k+kt}{c}\PY{p}{(}orange\PY{p}{,}blue\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{green\PYZsq{}}\PY{p}{)}\PY{p}{,} cex\PY{o}{=}\PY{l+m}{0.7}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

 iter imp variable
  1   1  fheight  sheight
  2   1  fheight  sheight
  3   1  fheight  sheight
  4   1  fheight  sheight
  5   1  fheight  sheight

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_99_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Data imputed using the simple random method compare similarly to the
linear regression imputation. Visually it is generally hard to
distinguish between the two methods. Perhaps random imputation has a
higher tendancy to imputate outlier values as seen with more blue points
located near the data extremes, thereby weakening the linear
relationship present in the data.

    \section{Extension}\label{extension}

Although the focus in this course is on the more traditional tools of
EDA, \emph{text mining} and the associated visualisation of word
frequencies through the use of \emph{word clouds} are becoming
increasingly prevalent.

In this extension, we will step through the basic process of reading in
text data, preparing text data for text mining, determining word
frequencies, and visualising the resulting frequencies in a word cloud.

Let's start by loading a few packages and some text data.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}45}]:} \PY{k+kn}{library}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{tm\PYZdq{}}\PY{p}{)} \PY{c+c1}{\PYZsh{} Text mining library}
         \PY{k+kn}{library}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{wordcloud\PYZdq{}}\PY{p}{)} \PY{c+c1}{\PYZsh{} Wordcloud plotting library}
         ulysses\PYZus{}raw\PY{o}{\PYZlt{}\PYZhy{}}\PY{k+kp}{readLines}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{./pg1727.txt\PYZdq{}}\PY{p}{)}
         ulysses\PYZus{}raw\PY{o}{\PYZlt{}\PYZhy{}}ulysses\PYZus{}raw\PY{p}{[}\PY{l+m}{342}\PY{o}{:}\PY{l+m}{10599}\PY{p}{]} \PY{c+c1}{\PYZsh{} Strip out front and back matter}
         ulysses\PY{o}{\PYZlt{}\PYZhy{}}Corpus\PY{p}{(}VectorSource\PY{p}{(}ulysses\PYZus{}raw\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} Convert to corpus format}
         inspect\PY{p}{(}ulysses\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{30}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{} Inspect first 30 lines}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Loading required package: NLP
Loading required package: RColorBrewer

    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
<<SimpleCorpus>>
Metadata:  corpus specific: 1, document level (indexed): 0
Content:  documents: 30

 [1] THE ODYSSEY                                                             
 [2]                                                                         
 [3]                                                                         
 [4] Book I                                                                  
 [5]                                                                         
 [6] THE GODS IN COUNCIL--MINERVA'S VISIT TO ITHACA--THE CHALLENGE FROM      
 [7] TELEMACHUS TO THE SUITORS.                                              
 [8]                                                                         
 [9] Tell me, O Muse, of that ingenious hero who travelled far and wide after
[10] he had sacked the famous town of Troy. Many cities did he visit, and    
[11] many were the nations with whose manners and customs he was acquainted; 
[12] moreover he suffered much by sea while trying to save his own life and  
[13] bring his men safely home; but do what he might he could not save his   
[14] men, for they perished through their own sheer folly in eating the      
[15] cattle of the Sun-god Hyperion; so the god prevented them from ever     
[16] reaching home. Tell me, too, about all these things, oh daughter of     
[17] Jove, from whatsoever source you may know them.                         
[18]                                                                         
[19] So now all who escaped death in battle or by shipwreck had got safely   
[20] home except Ulysses, and he, though he was longing to return to his wife
[21] and country, was detained by the goddess Calypso, who had got him into  
[22] a large cave and wanted to marry him. But as years went by, there came a
[23] time when the gods settled that he should go back to Ithaca; even then, 
[24] however, when he was among his own people, his troubles were not        
[25] yet over; nevertheless all the gods had now begun to pity him except    
[26] Neptune, who still persecuted him without ceasing and would not let him 
[27] get home.                                                               
[28]                                                                         
[29] Now Neptune had gone off to the Ethiopians, who are at the world's end, 
[30] and lie in two halves, the one looking West and the other East. \{1\} He  

    \end{Verbatim}

    To process the corpus and extract frequently used words (excluding
commonly used words known as \emph{stop words}), we need to remove
punctuation, numbers, case of type, stop words, and strip out any excess
whitespace.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}46}]:} ulysses \PY{o}{\PYZlt{}\PYZhy{}} tm\PYZus{}map\PY{p}{(}ulysses\PY{p}{,} content\PYZus{}transformer\PY{p}{(}\PY{k+kp}{tolower}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} Convert corpus to lower case}
         ulysses \PY{o}{\PYZlt{}\PYZhy{}} tm\PYZus{}map\PY{p}{(}ulysses\PY{p}{,} removePunctuation\PY{p}{)} \PY{c+c1}{\PYZsh{} Strip common punctuation}
         ulysses \PY{o}{\PYZlt{}\PYZhy{}} tm\PYZus{}map\PY{p}{(}ulysses\PY{p}{,} removeNumbers\PY{p}{)} \PY{c+c1}{\PYZsh{} Strip numbers }
         ulysses \PY{o}{\PYZlt{}\PYZhy{}} tm\PYZus{}map\PY{p}{(}ulysses\PY{p}{,} removeWords\PY{p}{,} stopwords\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{english\PYZdq{}}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} Strip default stop words}
         
         \PY{c+c1}{\PYZsh{} Strip custom stop words}
         ulysses \PY{o}{\PYZlt{}\PYZhy{}} tm\PYZus{}map\PY{p}{(}ulysses\PY{p}{,} removeWords\PY{p}{,} \PY{k+kt}{c}\PY{p}{(}\PY{l+s}{\PYZdq{}}\PY{l+s}{i\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{and\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{are\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{it\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{ii\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{iii\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{iv\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{v\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{vi\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{vii\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{viii\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{ix\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{x\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{xi\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{xii\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{xiii\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{xiv\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{xv\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{xvi\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{xvii\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{xviii\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{xix\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{xx\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{xxi\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{xxii\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{xxiii\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{xxiv\PYZdq{}}\PY{p}{,}\PY{l+s}{\PYZdq{}}\PY{l+s}{xxv\PYZdq{}}\PY{p}{)}\PY{p}{)} 
         
         ulysses \PY{o}{\PYZlt{}\PYZhy{}} tm\PYZus{}map\PY{p}{(}ulysses\PY{p}{,} stripWhitespace\PY{p}{)} \PY{c+c1}{\PYZsh{} Strip excess whitespace}
         
         inspect\PY{p}{(}ulysses\PY{p}{[}\PY{l+m}{1}\PY{o}{:}\PY{l+m}{30}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{} Inspect first 30 lines}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Warning message in tm\_map.SimpleCorpus(ulysses, content\_transformer(tolower)):
“transformation drops documents”Warning message in tm\_map.SimpleCorpus(ulysses, removePunctuation):
“transformation drops documents”Warning message in tm\_map.SimpleCorpus(ulysses, removeNumbers):
“transformation drops documents”Warning message in tm\_map.SimpleCorpus(ulysses, removeWords, stopwords("english")):
“transformation drops documents”Warning message in tm\_map.SimpleCorpus(ulysses, removeWords, c("i", "and", "are", :
“transformation drops documents”Warning message in tm\_map.SimpleCorpus(ulysses, stripWhitespace):
“transformation drops documents”
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
<<SimpleCorpus>>
Metadata:  corpus specific: 1, document level (indexed): 0
Content:  documents: 30

 [1]  odyssey                                        
 [2]                                                 
 [3]                                                 
 [4] book                                            
 [5]                                                 
 [6]  gods councilminervas visit ithacathe challenge 
 [7] telemachus suitors                              
 [8]                                                 
 [9] tell o muse ingenious hero travelled far wide   
[10]  sacked famous town troy many cities visit      
[11] many nations whose manners customs acquainted   
[12] moreover suffered much sea trying save life     
[13] bring men safely home might save                
[14] men perished sheer folly eating                 
[15] cattle sungod hyperion god prevented ever       
[16] reaching home tell things oh daughter           
[17] jove whatsoever source may know                 
[18]                                                 
[19]  now escaped death battle shipwreck got safely  
[20] home except ulysses though longing return wife  
[21]  country detained goddess calypso got           
[22]  large cave wanted marry years went came        
[23] time gods settled go back ithaca even           
[24] however among people troubles                   
[25] yet nevertheless gods now begun pity except     
[26] neptune still persecuted without ceasing let    
[27] get home                                        
[28]                                                 
[29] now neptune gone ethiopians worlds end          
[30]  lie two halves one looking west east           

    \end{Verbatim}

    Next we will create a \emph{term document matrix}, which counts word
occurence by corpus. Then we keep only the most frequent words.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}47}]:} ulysses\PYZus{}tdm \PY{o}{\PYZlt{}\PYZhy{}} TermDocumentMatrix\PY{p}{(}ulysses\PY{p}{)} \PY{c+c1}{\PYZsh{} Create a table counting word occurences by corpus}
         ulysses\PYZus{}tdm\PYZus{}sparse\PY{o}{\PYZlt{}\PYZhy{}}removeSparseTerms\PY{p}{(}ulysses\PYZus{}tdm \PY{p}{,} \PY{l+m}{0.995}\PY{p}{)} \PY{c+c1}{\PYZsh{} Keep only words that appear more than (1\PYZhy{}0.995)*10258 (around 51) times}
\end{Verbatim}


    From the sparse term document matrix, we will create word counts as
follows.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}48}]:} ulysses\PYZus{}tdms\PYZus{}matrix \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{as.matrix}\PY{p}{(}ulysses\PYZus{}tdm\PYZus{}sparse\PY{p}{)} \PY{c+c1}{\PYZsh{} Convert to matrix for processing}
         ulysses\PYZus{}tdms\PYZus{}freqs \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kp}{sort}\PY{p}{(}\PY{k+kp}{rowSums}\PY{p}{(}ulysses\PYZus{}tdms\PYZus{}matrix\PY{p}{)}\PY{p}{,}decreasing\PY{o}{=}\PY{k+kc}{TRUE}\PY{p}{)} \PY{c+c1}{\PYZsh{} Compute frequencies and sort}
         ulysses\PYZus{}tdms\PYZus{}freqs\PYZus{}df \PY{o}{\PYZlt{}\PYZhy{}} \PY{k+kt}{data.frame}\PY{p}{(}terms \PY{o}{=} \PY{k+kp}{names}\PY{p}{(}ulysses\PYZus{}tdms\PYZus{}freqs\PY{p}{)}\PY{p}{,}freq\PY{o}{=}ulysses\PYZus{}tdms\PYZus{}freqs\PY{p}{)} \PY{c+c1}{\PYZsh{} Construct data frame}
         \PY{k+kp}{head}\PY{p}{(}ulysses\PYZus{}tdms\PYZus{}freqs\PYZus{}df\PY{p}{,} \PY{l+m}{5}\PY{p}{)} \PY{c+c1}{\PYZsh{} Inspect the top 5 words}
\end{Verbatim}


    \begin{tabular}{r|ll}
  & terms & freq\\
\hline
	will & will    & 705    \\
	ulysses & ulysses & 593    \\
	one & one     & 511    \\
	said & said    & 478    \\
	house & house   & 394    \\
\end{tabular}


    
    Finally, we will plot the most common words in a \emph{word cloud}, in
which the size of the word is proportional to its frequency.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}49}]:} \PY{k+kp}{set.seed}\PY{p}{(}\PY{l+m}{8888}\PY{p}{)}
         wordcloud\PY{p}{(}words \PY{o}{=} ulysses\PYZus{}tdms\PYZus{}freqs\PYZus{}df\PY{o}{\PYZdl{}}terms\PY{p}{,} freq \PY{o}{=} ulysses\PYZus{}tdms\PYZus{}freqs\PYZus{}df\PY{o}{\PYZdl{}}freq\PY{p}{,} min.freq \PY{o}{=} \PY{l+m}{1}\PY{p}{,}max.words\PY{o}{=}\PY{l+m}{100}\PY{p}{,} random.order\PY{o}{=}\PY{k+kc}{FALSE}\PY{p}{,} random.color\PY{o}{=}\PY{k+kc}{FALSE}\PY{p}{,} rot.per\PY{o}{=}\PY{l+m}{0.4}\PY{p}{,}colors\PY{o}{=}brewer.pal\PY{p}{(}\PY{l+m}{8}\PY{p}{,} \PY{l+s}{\PYZdq{}}\PY{l+s}{Dark2\PYZdq{}}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_110_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    

    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
